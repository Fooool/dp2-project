{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harnessing Weather Insights for Accurate Energy Load Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: certifi==2024.12.14 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 7)) (2024.12.14)\n",
      "Requirement already satisfied: charset-normalizer==3.4.1 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 9)) (3.4.1)\n",
      "Requirement already satisfied: datetime==5.5 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 11)) (5.5)\n",
      "Requirement already satisfied: findspark==2.0.1 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 13)) (2.0.1)\n",
      "Requirement already satisfied: idna==3.10 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 15)) (3.10)\n",
      "Requirement already satisfied: numpy==2.2.2 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 17)) (2.2.2)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 19)) (0.10.9.7)\n",
      "Requirement already satisfied: pyspark==3.5.4 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 21)) (3.5.4)\n",
      "Requirement already satisfied: pytz==2024.2 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 23)) (2024.2)\n",
      "Requirement already satisfied: requests==2.32.3 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 25)) (2.32.3)\n",
      "Requirement already satisfied: urllib3==2.3.0 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 27)) (2.3.0)\n",
      "Requirement already satisfied: zope-interface==7.2 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 29)) (7.2)\n",
      "Requirement already satisfied: setuptools in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from zope-interface==7.2->-r requirements.txt (line 29)) (75.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windows OS detected\n"
     ]
    }
   ],
   "source": [
    "# Initialize via the full spark path\n",
    "if platform.system() == 'Windows':\n",
    "    print(\"Windows OS detected\")\n",
    "    findspark.init(\"C:/Spark/spark-3.5.4-bin-hadoop3\") # For my local machine\n",
    "else:\n",
    "    findspark.init(\"/usr/local/spark/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Linear Regression Model\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.databricks:spark-xml_2.12:0.15.0\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"100s\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "   \n",
    "# Main entry point for Spark functionality. A SparkContext represents the\n",
    "# connection to a Spark cluster, and can be used to create :class:`RDD` and\n",
    "# broadcast variables on that cluster.      \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing I: Read in Weather and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in data\\geosphere\\2022\\01.csv\n",
      "Read in 1490 rows\n",
      "Reading in data\\geosphere\\2022\\02.csv\n",
      "Read in 1346 rows\n",
      "Reading in data\\geosphere\\2022\\03.csv\n",
      "Read in 1490 rows\n",
      "Reading in data\\geosphere\\2022\\04.csv\n",
      "Read in 1442 rows\n",
      "Reading in data\\geosphere\\2022\\05.csv\n",
      "Read in 1490 rows\n",
      "Reading in data\\geosphere\\2022\\06.csv\n",
      "Read in 1442 rows\n",
      "Reading in data\\geosphere\\2022\\07.csv\n",
      "Read in 1490 rows\n",
      "Reading in data\\geosphere\\2022\\08.csv\n",
      "Read in 1490 rows\n",
      "Reading in data\\geosphere\\2022\\09.csv\n",
      "Read in 1442 rows\n",
      "Reading in data\\geosphere\\2022\\10.csv\n",
      "Read in 1490 rows\n",
      "Reading in data\\geosphere\\2022\\11.csv\n",
      "Read in 1442 rows\n",
      "Reading in data\\geosphere\\2022\\12.csv\n",
      "Read in 1490 rows\n",
      "Reading in data\\geosphere\\2023\\01.csv\n",
      "Read in 1490 rows\n",
      "Reading in data\\geosphere\\2023\\02.csv\n",
      "Read in 1346 rows\n",
      "Reading in data\\geosphere\\2023\\03.csv\n",
      "Read in 1490 rows\n",
      "Reading in data\\geosphere\\2023\\04.csv\n",
      "Read in 1442 rows\n",
      "Reading in data\\geosphere\\2023\\05.csv\n",
      "Read in 1490 rows\n",
      "Reading in data\\geosphere\\2023\\06.csv\n",
      "Read in 1442 rows\n",
      "Reading in data\\geosphere\\2023\\07.csv\n",
      "Read in 1490 rows\n",
      "Reading in data\\geosphere\\2023\\08.csv\n",
      "Read in 1490 rows\n",
      "Reading in data\\geosphere\\2023\\09.csv\n",
      "Read in 1442 rows\n",
      "Reading in data\\geosphere\\2023\\10.csv\n",
      "Read in 1490 rows\n",
      "Reading in data\\geosphere\\2023\\11.csv\n",
      "Read in 1442 rows\n",
      "Reading in data\\geosphere\\2023\\12.csv\n",
      "Read in 1490 rows\n",
      "Reading in data\\geosphere\\2024\\01.csv\n",
      "Read in 1490 rows\n",
      "Reading in data\\geosphere\\2024\\02.csv\n",
      "Read in 1394 rows\n",
      "Reading in data\\geosphere\\2024\\03.csv\n",
      "Read in 1490 rows\n",
      "Reading in data\\geosphere\\2024\\04.csv\n",
      "Read in 1442 rows\n",
      "Reading in data\\geosphere\\2024\\05.csv\n",
      "Read in 1490 rows\n",
      "Reading in data\\geosphere\\2024\\06.csv\n",
      "Read in 1442 rows\n",
      "Reading in data\\geosphere\\2024\\07.csv\n",
      "Read in 1490 rows\n",
      "Reading in data\\geosphere\\2024\\08.csv\n",
      "Read in 1490 rows\n",
      "Reading in data\\geosphere\\2024\\09.csv\n",
      "Read in 1442 rows\n",
      "Reading in data\\geosphere\\2024\\10.csv\n",
      "Read in 1490 rows\n",
      "Reading in data\\geosphere\\2024\\11.csv\n",
      "Read in 1442 rows\n",
      "Reading in data\\geosphere\\2024\\12.csv\n",
      "Read in 1490 rows\n",
      "+-------------------+------+------------------+-----------------+--------+------------------+\n",
      "|time               |avg_rr|avg_tl            |avg_p            |avg_so_h|avg_ff            |\n",
      "+-------------------+------+------------------+-----------------+--------+------------------+\n",
      "|2022-01-01 00:00:00|0.0   |13.15             |964.5999999999999|0.0     |7.65              |\n",
      "|2022-01-01 01:00:00|0.0   |12.75             |965.2            |0.0     |7.05              |\n",
      "|2022-01-01 02:00:00|0.0   |11.3              |965.75           |0.0     |5.3999999999999995|\n",
      "|2022-01-01 03:00:00|0.0   |11.100000000000001|965.75           |0.0     |3.5               |\n",
      "|2022-01-01 04:00:00|0.0   |11.95             |965.8            |0.0     |3.75              |\n",
      "|2022-01-01 05:00:00|0.0   |12.100000000000001|965.0999999999999|0.0     |4.5               |\n",
      "|2022-01-01 06:00:00|0.0   |12.3              |965.4            |0.0     |4.9               |\n",
      "|2022-01-01 07:00:00|0.0   |12.1              |966.0            |0.0     |4.35              |\n",
      "|2022-01-01 08:00:00|0.0   |12.35             |966.75           |0.55    |4.85              |\n",
      "|2022-01-01 09:00:00|0.0   |13.0              |967.1500000000001|0.75    |5.35              |\n",
      "+-------------------+------+------------------+-----------------+--------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "26305\n",
      "root\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- avg_rr: double (nullable = true)\n",
      " |-- avg_tl: double (nullable = true)\n",
      " |-- avg_p: double (nullable = true)\n",
      " |-- avg_so_h: double (nullable = true)\n",
      " |-- avg_ff: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in the data\n",
    "\n",
    "# Folder Structure\n",
    "# data\n",
    "# |-- geosphere\n",
    "# |   |-- YYYY\n",
    "# |      |-- MM.csv\n",
    "# |      |-- MM.csv\n",
    "# |\n",
    "# |-- transparency\n",
    "# |   |-- YYYY\n",
    "# |      |-- MM.xml\n",
    "# |      |-- MM.xml\n",
    "\n",
    "# Loop through the geosphere folder and read in the data\n",
    "\n",
    "# Define the base data folder\n",
    "base_path = Path(\"./data/geosphere\")\n",
    "\n",
    "# Collect all data frames first to optimize the union operation\n",
    "dfs = []\n",
    "\n",
    "for year_folder in base_path.iterdir():\n",
    "    if year_folder.is_dir():\n",
    "        for month_file in year_folder.glob(\"*.csv\"):\n",
    "            print(f\"Reading in {month_file}\")\n",
    "\n",
    "            df = spark.read.csv(str(month_file), header=True, inferSchema=True)\n",
    "\n",
    "            # Convert the time column (string) to a timestamp\n",
    "            df = df.withColumn(\"time\", to_timestamp(col(\"time\"), \"yyyy-MM-dd'T'HH:mmXXX\"))\n",
    "            \n",
    "            dfs.append(df)\n",
    "            \n",
    "            print(f\"Read in {df.count()} rows\")\n",
    "\n",
    "if dfs:\n",
    "    # Combine all DataFrames\n",
    "    weather = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        weather = weather.union(df)\n",
    "\n",
    "    # Aggregate measurements (average from different stations)\n",
    "    weather = (\n",
    "        weather.groupBy(\"time\")\n",
    "        .agg(\n",
    "            avg(\"rr\").alias(\"avg_rr\"),\n",
    "            avg(\"tl\").alias(\"avg_tl\"),\n",
    "            avg(\"p\").alias(\"avg_p\"),\n",
    "            avg(\"so_h\").alias(\"avg_so_h\"),\n",
    "            avg(\"ff\").alias(\"avg_ff\"),\n",
    "        )\n",
    "        .orderBy(\"time\")\n",
    "    )\n",
    "\n",
    "    weather.show(10, truncate=False)\n",
    "    \n",
    "    print(weather.count())\n",
    "    weather.printSchema()\n",
    "else:\n",
    "    print(\"No data found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading transparency data: data\\transparency\\2022\\01.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2022\\02.xml\n",
      "Read in 672 values\n",
      "Reading transparency data: data\\transparency\\2022\\03.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2022\\04.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data\\transparency\\2022\\05.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2022\\06.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data\\transparency\\2022\\07.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2022\\08.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2022\\09.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data\\transparency\\2022\\10.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2022\\11.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data\\transparency\\2022\\12.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2023\\01.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2023\\02.xml\n",
      "Read in 672 values\n",
      "Reading transparency data: data\\transparency\\2023\\03.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2023\\04.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data\\transparency\\2023\\05.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2023\\06.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data\\transparency\\2023\\07.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2023\\08.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2023\\09.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data\\transparency\\2023\\10.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2023\\11.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data\\transparency\\2023\\12.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2024\\01.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2024\\02.xml\n",
      "Read in 696 values\n",
      "Reading transparency data: data\\transparency\\2024\\03.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2024\\04.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data\\transparency\\2024\\05.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2024\\06.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data\\transparency\\2024\\07.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2024\\08.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2024\\09.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data\\transparency\\2024\\10.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2024\\11.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data\\transparency\\2024\\12.xml\n",
      "Read in 744 values\n",
      "+-------------------+--------+\n",
      "|          timestamp|quantity|\n",
      "+-------------------+--------+\n",
      "|2022-01-01 00:00:00| 22394.0|\n",
      "|2022-01-01 01:00:00| 21804.0|\n",
      "|2022-01-01 02:00:00| 20917.0|\n",
      "|2022-01-01 03:00:00| 20705.0|\n",
      "|2022-01-01 04:00:00| 20965.0|\n",
      "|2022-01-01 05:00:00| 21194.0|\n",
      "|2022-01-01 06:00:00| 22118.0|\n",
      "|2022-01-01 07:00:00| 23541.0|\n",
      "|2022-01-01 08:00:00| 24966.0|\n",
      "|2022-01-01 09:00:00| 25910.0|\n",
      "+-------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- quantity: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through the transparency folder and read in the energy data\n",
    "\n",
    "# Define base path for transparency data\n",
    "base_path = Path(\"./data/transparency\")\n",
    "\n",
    "# Collect DataFrames before performing union (optimization)\n",
    "dfs = []\n",
    "\n",
    "for year_folder in base_path.iterdir():\n",
    "    if year_folder.is_dir():\n",
    "        for month_file in year_folder.glob(\"*.xml\"):\n",
    "            print(f\"Reading transparency data: {month_file}\")\n",
    "\n",
    "            # Read XML data\n",
    "            df = spark.read.format('xml').option(\"rowTag\", \"GL_MarketDocument\").load(str(month_file))\n",
    "\n",
    "            # Extract and explode relevant fields\n",
    "            df_filtered = df.select(\n",
    "                col(\"TimeSeries.Period.timeInterval.start\").alias(\"start_time\"),\n",
    "                col(\"TimeSeries.Period.timeInterval.end\").alias(\"end_time\"),\n",
    "                col(\"TimeSeries.Period.resolution\").alias(\"resolution\"),\n",
    "                explode(col(\"TimeSeries.Period.Point\")).alias(\"Point\")  # Flatten Points\n",
    "            ).select(\n",
    "                col(\"start_time\"),\n",
    "                col(\"end_time\"),\n",
    "                col(\"resolution\"),\n",
    "                col(\"Point.position\").cast(\"int\").alias(\"position\"),\n",
    "                col(\"Point.quantity\").cast(\"double\").alias(\"quantity\")\n",
    "            )\n",
    "\n",
    "            # Convert ISO 8601 duration (e.g., \"PT15M\") to minutes dynamically\n",
    "            df_fixed = df_filtered.withColumn(\n",
    "                \"interval_minutes\",\n",
    "                expr(\"CAST(SUBSTRING(resolution, 3, LENGTH(resolution) - 3) AS INT)\")  # Extracts \"15\" from \"PT15M\"\n",
    "            ).withColumn(\n",
    "                \"actual_time\",\n",
    "                expr(\"start_time + (position - 1) * interval_minutes * interval 1 minute\")\n",
    "            ).select(\n",
    "                col(\"actual_time\"),\n",
    "                col(\"quantity\")\n",
    "            )\n",
    "            \n",
    "            # Aggregate to hourly intervals\n",
    "            df_hourly = df_fixed.withColumn(\n",
    "                \"hourly_time\", date_trunc(\"hour\", col(\"actual_time\"))  # Round down to the hour\n",
    "            ).groupBy(\"hourly_time\").agg(\n",
    "                sum(\"quantity\").alias(\"hourly_quantity\")  # Sum all sub-hourly measurements\n",
    "            )\n",
    "\n",
    "            # Rename to match the structure of other time series\n",
    "            df_hourly = df_hourly.select(\n",
    "                col(\"hourly_time\").alias(\"timestamp\"), col(\"hourly_quantity\").alias(\"quantity\")\n",
    "            ).orderBy(\"timestamp\")\n",
    "            \n",
    "            print(f\"Read in {df_hourly.count()} values\")\n",
    "            \n",
    "            # Append DataFrame to list\n",
    "            dfs.append(df_hourly)\n",
    "\n",
    "# Merge all collected DataFrames\n",
    "if dfs:\n",
    "    Load = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        Load = Load.union(df)\n",
    "\n",
    "    Load.show(10)\n",
    "    Load.printSchema()\n",
    "else:\n",
    "    print(\"No data found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing II: Combine both Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+------------------+-----------------+------------------+------------------+\n",
      "|summary|             load|           rainfall|       temperature|         pressure| sunshine_duration|        wind_speed|\n",
      "+-------+-----------------+-------------------+------------------+-----------------+------------------+------------------+\n",
      "|  count|            26304|              26304|             26304|            26304|             26304|             26304|\n",
      "|   mean|27147.53615419708|0.09834435827250564|10.736714948296894|959.9205348996387|0.2120685066909985| 2.136152296228693|\n",
      "| stddev|5198.551548542342| 0.6008211250333243|   8.2578055043952|7.875779934448332|0.3395092943434039|1.3306188964835226|\n",
      "|    min|          16055.0|                0.0|             -11.0|            929.4|               0.0|              0.05|\n",
      "|    max|          41273.0|               46.3|              33.8|            982.8|               1.0|             10.35|\n",
      "+-------+-----------------+-------------------+------------------+-----------------+------------------+------------------+\n",
      "\n",
      "+-------+-------------------+--------+------------------+-----------------+-----------------+------------------+\n",
      "|   load|               time|rainfall|       temperature|         pressure|sunshine_duration|        wind_speed|\n",
      "+-------+-------------------+--------+------------------+-----------------+-----------------+------------------+\n",
      "|20965.0|2022-01-01 04:00:00|     0.0|             11.95|            965.8|              0.0|              3.75|\n",
      "|27933.0|2022-01-02 10:00:00|     0.0|2.6999999999999997|            964.4|             0.55|0.8500000000000001|\n",
      "|26437.0|2022-01-02 12:00:00|     0.0|               6.1|           962.65|             0.95|              0.75|\n",
      "|25867.0|2022-01-02 13:00:00|     0.0|              7.25|962.0999999999999|             0.65|               0.7|\n",
      "|21901.0|2022-01-03 00:00:00|     0.0|               3.4|            959.2|              0.0|0.7000000000000001|\n",
      "|32966.0|2022-01-03 12:00:00|     0.0|             11.65|           958.15|             0.55|               5.3|\n",
      "|31724.0|2022-01-03 18:00:00|     0.0| 8.350000000000001|            957.7|              0.0|              3.55|\n",
      "|23632.0|2022-01-04 00:00:00|     0.0|5.3999999999999995|           954.45|              0.0|               0.8|\n",
      "|27724.0|2022-01-04 05:00:00|     0.0| 5.699999999999999|950.3499999999999|              0.0|               1.1|\n",
      "|33505.0|2022-01-04 08:00:00|    0.05|              4.85|           948.65|              0.0|1.5499999999999998|\n",
      "+-------+-------------------+--------+------------------+-----------------+-----------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- load: double (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- rainfall: double (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- sunshine_duration: double (nullable = true)\n",
      " |-- wind_speed: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if Load is not None and weather is not None:\n",
    "    # Join the data into a single DataFrame\n",
    "    data = Load.join(weather, Load.timestamp == weather.time, \"inner\").drop(\"time\")\n",
    "    \n",
    "    # Rename columns for better understanding\n",
    "    data = data.withColumnRenamed(\"timestamp\", \"time\")\n",
    "    data = data.withColumnRenamed(\"quantity\", \"load\")\n",
    "\n",
    "    data = data.withColumnRenamed(\"avg_rr\", \"rainfall\")\n",
    "    data = data.withColumnRenamed(\"avg_tl\", \"temperature\")\n",
    "    data = data.withColumnRenamed(\"avg_p\", \"pressure\")\n",
    "    data = data.withColumnRenamed(\"avg_so_h\", \"sunshine_duration\")\n",
    "    data = data.withColumnRenamed(\"avg_ff\", \"wind_speed\")\n",
    "    \n",
    "    # Reorder the columns\n",
    "    data = data.select(\"load\", \"time\", \"rainfall\", \"temperature\", \"pressure\", \"sunshine_duration\", \"wind_speed\")\n",
    "    \n",
    "    # Print the schema and stats\n",
    "    data.describe().show()\n",
    "    data.show(10)\n",
    "    data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|load   |features                                                                                                                                 |\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|20965.0|[0.017213396404173065,0.9998518385162037,0.8660266281835433,0.4999978792725455,0.0,0.0,11.95,965.8,0.0,3.75]                             |\n",
      "|27933.0|[0.034421692083641306,0.9994073979684656,0.4999946981757423,-0.8660284647724625,0.0,0.0,2.6999999999999997,964.4,0.55,0.8500000000000001]|\n",
      "|26437.0|[0.034421692083641306,0.9994073979684656,-7.346410206643587E-6,-0.9999999999730151,0.0,0.0,6.1,962.65,0.95,0.75]                         |\n",
      "|25867.0|[0.034421692083641306,0.9994073979684656,-0.2588267325222856,-0.9659237664183635,0.0,0.0,7.25,962.0999999999999,0.65,0.7]                |\n",
      "|21901.0|[0.05161978782516175,0.9986668100547276,0.0,1.0,0.0,0.0,3.4,959.2,0.0,0.7000000000000001]                                                |\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Keep original timestamp before converting to Unix seconds\n",
    "unix_time_data = data.withColumn(\"unix_time\", unix_timestamp(\"time\"))  # Store Unix time separately\n",
    "\n",
    "# Extract day of the year (1-365/366) from the original timestamp column\n",
    "unix_time_data = unix_time_data.withColumn(\"day_of_year\", dayofyear(col(\"time\")))\n",
    "\n",
    "# Extract time of day in seconds (seconds since midnight)\n",
    "unix_time_data = unix_time_data.withColumn(\n",
    "    \"time_of_day\", expr(\"hour(time) * 3600 + minute(time) * 60 + second(time)\")\n",
    ")\n",
    "\n",
    "# Normalize day of the year to range (0, 2Ï€) for periodic encoding\n",
    "ml_data = unix_time_data.withColumn(\"day_sin\", sin(2 * 3.1416 * col(\"day_of_year\") / 365))\n",
    "ml_data = ml_data.withColumn(\"day_cos\", cos(2 * 3.1416 * col(\"day_of_year\") / 365))\n",
    "\n",
    "# Normalize time of day\n",
    "ml_data = ml_data.withColumn(\"time_sin\", sin(2 * 3.1416 * col(\"time_of_day\") / 86400))\n",
    "ml_data = ml_data.withColumn(\"time_cos\", cos(2 * 3.1416 * col(\"time_of_day\") / 86400))\n",
    "\n",
    "# Add weekend feature\n",
    "ml_data = ml_data.withColumn(\"is_weekend\", when(col(\"time\").cast(\"string\").like(\"%Sat%\") | col(\"time\").cast(\"string\").like(\"%Sun%\"), lit(1)).otherwise(lit(0)))\n",
    "\n",
    "# Define new feature columns\n",
    "feature_columns = [\"day_sin\", \"day_cos\", \"time_sin\", \"time_cos\", \"is_weekend\", \"rainfall\", \"temperature\", \"pressure\", \"sunshine_duration\", \"wind_speed\"]\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "# Transform data\n",
    "df_ml = assembler.transform(ml_data).select(\"load\", \"features\")\n",
    "\n",
    "df_ml.cache()\n",
    "\n",
    "# Show transformed data\n",
    "df_ml.show(5, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|load   |features                                                                                                                                                            |\n",
      "+-------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|20965.0|[0.024354090758037714,1.4133324448072522,1.2247247212238557,0.7070895331403335,0.0,0.0,1.4471157008529243,122.62912473920099,0.0,2.8182374456805577]                |\n",
      "|27933.0|[0.048700964839629,1.4127042094810678,0.7070866500041507,-1.2247245202982293,0.0,0.0,0.3269633801090289,122.4513645666654,1.6199851054554413,0.6388004876875932]    |\n",
      "|26437.0|[0.048700964839629,1.4127042094810678,-1.0389207328646696E-5,-1.4141850644446894,0.0,0.0,0.7386950439500283,122.2291643509959,2.7981560912412164,0.5636474891361116]|\n",
      "|25867.0|[0.048700964839629,1.4127042094810678,-0.36602973571207026,-1.3659949638978717,0.0,0.0,0.877957224366837,122.15932999749977,1.9145278519018851,0.5260709898603707]  |\n",
      "|21901.0|[0.07303340770679437,1.4116573574511997,0.0,1.4141850644828509,0.0,0.0,0.4117316638409994,121.79111249724747,0.0,0.5260709898603708]                                |\n",
      "+-------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "scaled_df = scaler.fit(df_ml).transform(df_ml).select(\"load\", \"features_scaled\")\n",
    "scaled_df = scaled_df.withColumnRenamed(\"features_scaled\", \"features\")\n",
    "scaled_df.cache()\n",
    "\n",
    "scaled_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(load=16209.0, features=DenseVector([0.3258, -1.3755, 0.366, 1.366, 0.0, 0.0, 1.3866, 121.8546, 0.0, 0.1127])),\n",
       " Row(load=16675.0, features=DenseVector([0.3258, -1.3755, 1.2247, 0.7071, 0.0, 0.0, 1.3139, 121.8863, 0.7364, 0.2255])),\n",
       " Row(load=16709.0, features=DenseVector([-1.3754, -0.3314, 0.7071, 1.2247, 0.0, 0.0, 1.7196, 122.0324, 0.0, 0.3758])),\n",
       " Row(load=16946.0, features=DenseVector([0.6666, -1.2468, 0.7071, 1.2247, 0.0, 0.1664, 1.435, 121.4864, 0.0, 2.5928])),\n",
       " Row(load=17009.0, features=DenseVector([0.9452, -1.0518, 0.366, 1.366, 0.0, 0.0, 1.4471, 121.1943, 0.0, 0.977]))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly splits this :class:`DataFrame` with the provided weights.\n",
    "train_data, test_data = scaled_df.randomSplit([.8,.2],seed=1234)\n",
    "\n",
    "train_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression.\n",
    "# The learning objective is to minimize the specified loss function, with regularization.\n",
    "#More about regParam and elasticNetParam (seen as penalties on the loss function that minimises error in predicitons) can be found here: https://runawayhorse001.github.io/LearningApacheSpark/reg.html\n",
    "# The maxIter parameter sets an upper limit on the number of iterations the optimization algorithm can perform regarding the loss function\n",
    "# By default the LinearRegression function expects another column named \"features\"\n",
    "\n",
    "lr = LinearRegression(labelCol=\"load\", maxIter=500)\n",
    "\n",
    "# Fits a model to the input dataset with optional parameters.\n",
    "linearModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- load: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n",
      "+-------+--------------------+------------------+\n",
      "|   load|            features|        prediction|\n",
      "+-------+--------------------+------------------+\n",
      "|16216.0|[0.77141735209955...| 22068.06741807801|\n",
      "|16826.0|[0.32583482286144...|21137.598146455763|\n",
      "|17352.0|[-0.9808868570328...| 20643.40006582645|\n",
      "|17364.0|[-0.4889947446018...| 20674.85523010025|\n",
      "|17379.0|[0.18214735722752...|20219.906228724216|\n",
      "|17396.0|[-1.3255362000285...|21357.664270566158|\n",
      "|17495.0|[-0.1579896063647...| 21126.03854991275|\n",
      "|17708.0|[-1.0649069797603...|20066.880084899847|\n",
      "|17850.0|[-0.9631904884983...|21890.241968869013|\n",
      "|17850.0|[0.94519323616894...| 23837.97097647786|\n",
      "|17896.0|[-0.7301528247730...| 20915.24371877881|\n",
      "|17983.0|[0.18214735722752...| 21068.31012799561|\n",
      "|17996.0|[0.92693112836009...|23676.099723102532|\n",
      "|18010.0|[1.03219724640567...|23280.410296724294|\n",
      "|18070.0|[-1.4147158374425...|22922.517543062786|\n",
      "|18140.0|[-1.3694745558284...| 21965.61388397192|\n",
      "|18221.0|[-0.9084102867700...|21959.179211421248|\n",
      "|18243.0|[0.90839434956854...|22652.362726113322|\n",
      "|18305.0|[0.62325277972926...| 21039.14522186727|\n",
      "|18422.0|[-0.7714347781549...|21801.208166253866|\n",
      "+-------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform  is used to perform value predictions using the trained model\n",
    "predicted = linearModel.transform(test_data)\n",
    "predicted.printSchema()\n",
    "\n",
    "predicted.select(\"load\",\"features\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 3560.167659\n",
      "r2: 0.527683\n",
      "Adjur2: 0.527459\n",
      "MSE: 12674793.760845\n",
      "MAE: 2876.655052\n"
     ]
    }
   ],
   "source": [
    "# regression Summary\n",
    "trainingSummary = linearModel.summary\n",
    "\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)\n",
    "print(\"Adjur2: %f\" % trainingSummary.r2adj)\n",
    "print(\"MSE: %f\" % trainingSummary.meanSquaredError)\n",
    "print(\"MAE: %f\" % trainingSummary.meanAbsoluteError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
