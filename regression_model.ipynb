{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harnessing Weather Insights for Accurate Energy Load Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize via the full spark path\n",
    "if platform.system() == 'Windows':\n",
    "    print(\"Windows OS detected\")\n",
    "    findspark.init(\"C:/Spark/spark-3.5.4-bin-hadoop3\") # For my local machine\n",
    "else:\n",
    "    findspark.init(\"/usr/local/spark/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local\") \\\n",
    "      .appName(\"Linear Regression Model\") \\\n",
    "      .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "      .config(\"spark.jars.packages\", \"com.databricks:spark-xml_2.12:0.15.0\") \\\n",
    "      .getOrCreate()\n",
    "   \n",
    "# Main entry point for Spark functionality. A SparkContext represents the\n",
    "# connection to a Spark cluster, and can be used to create :class:`RDD` and\n",
    "# broadcast variables on that cluster.      \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing I: Read Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "\n",
    "# Folder Structure\n",
    "# data\n",
    "# |-- geosphere\n",
    "# |   |-- YYYY\n",
    "# |      |-- MM.csv\n",
    "# |      |-- MM.csv\n",
    "# |\n",
    "# |-- transparency\n",
    "# |   |-- YYYY\n",
    "# |      |-- MM.xml\n",
    "# |      |-- MM.xml\n",
    "\n",
    "# Loop through the geosphere folder and read in the data\n",
    "\n",
    "weather = None\n",
    "\n",
    "for year_folder in os.listdir(\"./data/geosphere/\"):\n",
    "    year_path = os.path.join(\"./data/geosphere/\", year_folder)\n",
    "    \n",
    "    if os.path.isdir(year_path):\n",
    "        for month_file in os.listdir(year_path):\n",
    "            if month_file.endswith(\".csv\"):\n",
    "                filepath = os.path.join(year_path, month_file)\n",
    "\n",
    "                df = spark.read.csv(filepath, header=True, inferSchema=True)\n",
    "                \n",
    "                # Combine the data\n",
    "                if weather is None:\n",
    "                    weather = df\n",
    "                else:\n",
    "                    weather = weather.union(df)\n",
    "\n",
    "if weather is not None:\n",
    "    weather.show(100)\n",
    "    weather.printSchema()\n",
    "else:\n",
    "    print(\"No data found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing II: Read Energy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the transparency folder and read in the energy data\n",
    "\n",
    "Load = None  # Initialize empty DataFrame\n",
    "\n",
    "# Loop through transparency data folder\n",
    "for year_folder in os.listdir(\"./data/transparency/\"):\n",
    "    year_path = os.path.join(\"./data/transparency/\", year_folder)\n",
    "\n",
    "    if os.path.isdir(year_path):\n",
    "        for month_file in os.listdir(year_path):\n",
    "            if month_file.endswith(\".xml\"):\n",
    "                filepath = os.path.join(year_path, month_file)\n",
    "                print(f\"Reading transparency data: {filepath}\")\n",
    "\n",
    "                # Read XML data\n",
    "                df = spark.read.format('xml').option(\"rowTag\", \"GL_MarketDocument\").load(filepath)\n",
    "\n",
    "                # Extract and explode data\n",
    "                df_filtered = df.select(\n",
    "                    col(\"TimeSeries.Period.timeInterval.start\").alias(\"start_time\"),\n",
    "                    col(\"TimeSeries.Period.timeInterval.end\").alias(\"end_time\"),\n",
    "                    col(\"TimeSeries.Period.resolution\").alias(\"resolution\"),\n",
    "                    explode(col(\"TimeSeries.Period.Point\")).alias(\"Point\")  # Flatten Points\n",
    "                ).select(\n",
    "                    col(\"start_time\"),\n",
    "                    col(\"end_time\"),\n",
    "                    col(\"resolution\"),\n",
    "                    col(\"Point.position\").cast(\"int\").alias(\"position\"),\n",
    "                    col(\"Point.quantity\").cast(\"double\").alias(\"quantity\")\n",
    "                )\n",
    "\n",
    "                # Use resolution dynamically (assuming all values follow ISO 8601 duration format)\n",
    "                df_fixed = df_filtered.withColumn(\n",
    "                    \"actual_time\",\n",
    "                    expr(\"start_time + (position - 1) * interval 15 minutes\")  # Change to dynamic interval if needed\n",
    "                ).select(\n",
    "                    col(\"actual_time\"),\n",
    "                    col(\"position\"),\n",
    "                    col(\"quantity\")\n",
    "                )\n",
    "                \n",
    "                # Append to Load DataFrame\n",
    "                if Load is None:\n",
    "                    Load = df_fixed\n",
    "                else:\n",
    "                    Load = Load.union(df_fixed)\n",
    "\n",
    "# Show final merged DataFrame\n",
    "if Load is not None:\n",
    "    Load.show(1000)\n",
    "    Load.printSchema()\n",
    "else:\n",
    "    print(\"No data found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
