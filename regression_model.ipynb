{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harnessing Weather Insights for Accurate Energy Load Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: certifi==2024.12.14 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 7)) (2024.12.14)\n",
      "Requirement already satisfied: charset-normalizer==3.4.1 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 9)) (3.4.1)\n",
      "Requirement already satisfied: datetime==5.5 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 11)) (5.5)\n",
      "Requirement already satisfied: findspark==2.0.1 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 13)) (2.0.1)\n",
      "Requirement already satisfied: idna==3.10 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 15)) (3.10)\n",
      "Requirement already satisfied: numpy==2.2.2 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 17)) (2.2.2)\n",
      "Requirement already satisfied: pandas==2.2.3 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 21)) (2.2.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 23)) (0.10.9.7)\n",
      "Requirement already satisfied: pyspark==3.5.4 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 25)) (3.5.4)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 27)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz==2024.2 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 29)) (2024.2)\n",
      "Requirement already satisfied: requests==2.32.3 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 33)) (2.32.3)\n",
      "Requirement already satisfied: six==1.17.0 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 35)) (1.17.0)\n",
      "Requirement already satisfied: tzdata==2025.1 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 37)) (2025.1)\n",
      "Requirement already satisfied: urllib3==2.3.0 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 39)) (2.3.0)\n",
      "Requirement already satisfied: zope-interface==7.2 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 41)) (7.2)\n",
      "Requirement already satisfied: setuptools in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from zope-interface==7.2->-r requirements.txt (line 41)) (75.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windows OS detected\n"
     ]
    }
   ],
   "source": [
    "# Initialize via the full spark path\n",
    "if platform.system() == 'Windows':\n",
    "    print(\"Windows OS detected\")\n",
    "    findspark.init(\"C:/Spark/spark-3.5.4-bin-hadoop3\") # For my local machine\n",
    "else:\n",
    "    findspark.init(\"/usr/local/spark/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Linear Regression Model\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.databricks:spark-xml_2.12:0.15.0\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"100s\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "   \n",
    "# Main entry point for Spark functionality. A SparkContext represents the\n",
    "# connection to a Spark cluster, and can be used to create :class:`RDD` and\n",
    "# broadcast variables on that cluster.      \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing I: Read in Weather and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in data\\geosphere\\2022\\01.csv\n",
      "Read in 8940 rows\n",
      "Reading in data\\geosphere\\2022\\02.csv\n",
      "Read in 8076 rows\n",
      "Reading in data\\geosphere\\2022\\03.csv\n",
      "Read in 8940 rows\n",
      "Reading in data\\geosphere\\2022\\04.csv\n",
      "Read in 8652 rows\n",
      "Reading in data\\geosphere\\2022\\05.csv\n",
      "Read in 8940 rows\n",
      "Reading in data\\geosphere\\2022\\06.csv\n",
      "Read in 8652 rows\n",
      "Reading in data\\geosphere\\2022\\07.csv\n",
      "Read in 8940 rows\n",
      "Reading in data\\geosphere\\2022\\08.csv\n",
      "Read in 8940 rows\n",
      "Reading in data\\geosphere\\2022\\09.csv\n",
      "Read in 8652 rows\n",
      "Reading in data\\geosphere\\2022\\10.csv\n",
      "Read in 8940 rows\n",
      "Reading in data\\geosphere\\2022\\11.csv\n",
      "Read in 8652 rows\n",
      "Reading in data\\geosphere\\2022\\12.csv\n",
      "Read in 8940 rows\n",
      "Reading in data\\geosphere\\2023\\01.csv\n",
      "Read in 8940 rows\n",
      "Reading in data\\geosphere\\2023\\02.csv\n",
      "Read in 8076 rows\n",
      "Reading in data\\geosphere\\2023\\03.csv\n",
      "Read in 8940 rows\n",
      "Reading in data\\geosphere\\2023\\04.csv\n",
      "Read in 8652 rows\n",
      "Reading in data\\geosphere\\2023\\05.csv\n",
      "Read in 8940 rows\n",
      "Reading in data\\geosphere\\2023\\06.csv\n",
      "Read in 8652 rows\n",
      "Reading in data\\geosphere\\2023\\07.csv\n",
      "Read in 8940 rows\n",
      "Reading in data\\geosphere\\2023\\08.csv\n",
      "Read in 8940 rows\n",
      "Reading in data\\geosphere\\2023\\09.csv\n",
      "Read in 8652 rows\n",
      "Reading in data\\geosphere\\2023\\10.csv\n",
      "Read in 8940 rows\n",
      "Reading in data\\geosphere\\2023\\11.csv\n",
      "Read in 8652 rows\n",
      "Reading in data\\geosphere\\2023\\12.csv\n",
      "Read in 8940 rows\n",
      "Reading in data\\geosphere\\2024\\01.csv\n",
      "Read in 8940 rows\n",
      "Reading in data\\geosphere\\2024\\02.csv\n",
      "Read in 8364 rows\n",
      "Reading in data\\geosphere\\2024\\03.csv\n",
      "Read in 8940 rows\n",
      "Reading in data\\geosphere\\2024\\04.csv\n",
      "Read in 8652 rows\n",
      "Reading in data\\geosphere\\2024\\05.csv\n",
      "Read in 8940 rows\n",
      "Reading in data\\geosphere\\2024\\06.csv\n",
      "Read in 8652 rows\n",
      "Reading in data\\geosphere\\2024\\07.csv\n",
      "Read in 8940 rows\n",
      "Reading in data\\geosphere\\2024\\08.csv\n",
      "Read in 8940 rows\n",
      "Reading in data\\geosphere\\2024\\09.csv\n",
      "Read in 8652 rows\n",
      "Reading in data\\geosphere\\2024\\10.csv\n",
      "Read in 8940 rows\n",
      "Reading in data\\geosphere\\2024\\11.csv\n",
      "Read in 8652 rows\n",
      "Reading in data\\geosphere\\2024\\12.csv\n",
      "Read in 8940 rows\n",
      "+-------------------+------+-----------------+-----------------+--------------------+------------------+------+------------------+\n",
      "|time               |avg_rr|avg_tl           |avg_p            |avg_so_h            |avg_ff            |log_rr|log_ff            |\n",
      "+-------------------+------+-----------------+-----------------+--------------------+------------------+------+------------------+\n",
      "|2022-01-01 00:00:00|0.0   |8.100000000000001|978.3727272727273|0.0                 |4.40909090909091  |0.0   |1.6880810397532138|\n",
      "|2022-01-01 01:00:00|0.0   |8.454545454545453|978.7            |0.0                 |3.909090909090909 |0.0   |1.5910887737659039|\n",
      "|2022-01-01 02:00:00|0.0   |7.527272727272727|979.1454545454545|0.0                 |3.4545454545454546|0.0   |1.493925025312256 |\n",
      "|2022-01-01 03:00:00|0.0   |7.072727272727272|979.4            |0.0                 |2.6545454545454543|0.0   |1.2959717228266048|\n",
      "|2022-01-01 04:00:00|0.0   |7.300000000000001|979.6272727272727|0.0                 |2.709090909090909 |0.0   |1.3107868086117453|\n",
      "|2022-01-01 05:00:00|0.0   |7.318181818181818|979.7363636363639|0.0                 |3.290909090909091 |0.0   |1.456498619793139 |\n",
      "|2022-01-01 06:00:00|0.0   |6.963636363636365|979.9636363636363|0.0                 |3.027272727272727 |0.0   |1.3930894042527193|\n",
      "|2022-01-01 07:00:00|0.0   |6.818181818181818|980.3454545454545|0.009090909090909092|3.127272727272727 |0.0   |1.4176168322489318|\n",
      "|2022-01-01 08:00:00|0.0   |7.627272727272728|980.7363636363636|0.2727272727272727  |3.409090909090909 |0.0   |1.483668525145067 |\n",
      "|2022-01-01 09:00:00|0.0   |8.245454545454546|980.9272727272728|0.4181818181818182  |3.3               |0.0   |1.4586150226995167|\n",
      "+-------------------+------+-----------------+-----------------+--------------------+------------------+------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "26305\n",
      "root\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- avg_rr: double (nullable = true)\n",
      " |-- avg_tl: double (nullable = true)\n",
      " |-- avg_p: double (nullable = true)\n",
      " |-- avg_so_h: double (nullable = true)\n",
      " |-- avg_ff: double (nullable = true)\n",
      " |-- log_rr: double (nullable = true)\n",
      " |-- log_ff: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in the data\n",
    "\n",
    "# Folder Structure\n",
    "# data\n",
    "# |-- geosphere\n",
    "# |   |-- YYYY\n",
    "# |      |-- MM.csv\n",
    "# |      |-- MM.csv\n",
    "# |\n",
    "# |-- transparency\n",
    "# |   |-- YYYY\n",
    "# |      |-- MM.xml\n",
    "# |      |-- MM.xml\n",
    "\n",
    "# Loop through the geosphere folder and read in the data\n",
    "\n",
    "# Define the base data folder\n",
    "base_path = Path(\"./data/geosphere\")\n",
    "\n",
    "# Collect all data frames first to optimize the union operation\n",
    "dfs = []\n",
    "\n",
    "for year_folder in base_path.iterdir():\n",
    "    if year_folder.is_dir():\n",
    "        for month_file in year_folder.glob(\"*.csv\"):\n",
    "            print(f\"Reading in {month_file}\")\n",
    "\n",
    "            df = spark.read.csv(str(month_file), header=True, inferSchema=True)\n",
    "\n",
    "            # Convert the time column (string) to a timestamp\n",
    "            df = df.withColumn(\"time\", to_timestamp(col(\"time\"), \"yyyy-MM-dd'T'HH:mmXXX\"))\n",
    "            \n",
    "            dfs.append(df)\n",
    "            \n",
    "            print(f\"Read in {df.count()} rows\")\n",
    "\n",
    "if dfs:\n",
    "    # Combine all DataFrames\n",
    "    weather = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        weather = weather.union(df)\n",
    "\n",
    "    # Aggregate measurements (average from different stations)\n",
    "    weather = (\n",
    "        weather.groupBy(\"time\")\n",
    "        .agg(\n",
    "            avg(\"rr\").alias(\"avg_rr\"),\n",
    "            avg(\"tl\").alias(\"avg_tl\"),\n",
    "            avg(\"p\").alias(\"avg_p\"),\n",
    "            avg(\"so_h\").alias(\"avg_so_h\"),\n",
    "            avg(\"ff\").alias(\"avg_ff\"),\n",
    "        )\n",
    "        .orderBy(\"time\")\n",
    "    )\n",
    "\n",
    "    # Log-transform the measurements (data for rainfall and windspeed is skewed)\n",
    "    weather = weather.withColumn(\"log_rr\", log1p(col(\"avg_rr\")))\n",
    "    weather = weather.withColumn(\"log_ff\", log1p(col(\"avg_ff\")))\n",
    "    \n",
    "    weather.drop(\"avg_rr\", \"avg_ff\")\n",
    "    \n",
    "    weather.show(10, truncate=False)\n",
    "    \n",
    "    print(weather.count())\n",
    "    weather.printSchema()\n",
    "else:\n",
    "    print(\"No data found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading transparency data: data\\transparency\\2022\\01.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2022\\02.xml\n",
      "Read in 672 values\n",
      "Reading transparency data: data\\transparency\\2022\\03.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2022\\04.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data\\transparency\\2022\\05.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2022\\06.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data\\transparency\\2022\\07.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2022\\08.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2022\\09.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data\\transparency\\2022\\10.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2022\\11.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data\\transparency\\2022\\12.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2023\\01.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2023\\02.xml\n",
      "Read in 672 values\n",
      "Reading transparency data: data\\transparency\\2023\\03.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2023\\04.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data\\transparency\\2023\\05.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2023\\06.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data\\transparency\\2023\\07.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2023\\08.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2023\\09.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data\\transparency\\2023\\10.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2023\\11.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data\\transparency\\2023\\12.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2024\\01.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2024\\02.xml\n",
      "Read in 696 values\n",
      "Reading transparency data: data\\transparency\\2024\\03.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2024\\04.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data\\transparency\\2024\\05.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2024\\06.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data\\transparency\\2024\\07.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2024\\08.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2024\\09.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data\\transparency\\2024\\10.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data\\transparency\\2024\\11.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data\\transparency\\2024\\12.xml\n",
      "Read in 744 values\n",
      "+-------------------+--------+\n",
      "|          timestamp|quantity|\n",
      "+-------------------+--------+\n",
      "|2022-01-01 00:00:00| 22394.0|\n",
      "|2022-01-01 01:00:00| 21804.0|\n",
      "|2022-01-01 02:00:00| 20917.0|\n",
      "|2022-01-01 03:00:00| 20705.0|\n",
      "|2022-01-01 04:00:00| 20965.0|\n",
      "|2022-01-01 05:00:00| 21194.0|\n",
      "|2022-01-01 06:00:00| 22118.0|\n",
      "|2022-01-01 07:00:00| 23541.0|\n",
      "|2022-01-01 08:00:00| 24966.0|\n",
      "|2022-01-01 09:00:00| 25910.0|\n",
      "+-------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- quantity: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through the transparency folder and read in the energy data\n",
    "\n",
    "# Define base path for transparency data\n",
    "base_path = Path(\"./data/transparency\")\n",
    "\n",
    "# Collect DataFrames before performing union (optimization)\n",
    "dfs = []\n",
    "\n",
    "for year_folder in base_path.iterdir():\n",
    "    if year_folder.is_dir():\n",
    "        for month_file in year_folder.glob(\"*.xml\"):\n",
    "            print(f\"Reading transparency data: {month_file}\")\n",
    "\n",
    "            # Read XML data\n",
    "            df = spark.read.format('xml').option(\"rowTag\", \"GL_MarketDocument\").load(str(month_file))\n",
    "\n",
    "            # Extract and explode relevant fields\n",
    "            df_filtered = df.select(\n",
    "                col(\"TimeSeries.Period.timeInterval.start\").alias(\"start_time\"),\n",
    "                col(\"TimeSeries.Period.timeInterval.end\").alias(\"end_time\"),\n",
    "                col(\"TimeSeries.Period.resolution\").alias(\"resolution\"),\n",
    "                explode(col(\"TimeSeries.Period.Point\")).alias(\"Point\")  # Flatten Points\n",
    "            ).select(\n",
    "                col(\"start_time\"),\n",
    "                col(\"end_time\"),\n",
    "                col(\"resolution\"),\n",
    "                col(\"Point.position\").cast(\"int\").alias(\"position\"),\n",
    "                col(\"Point.quantity\").cast(\"double\").alias(\"quantity\")\n",
    "            )\n",
    "\n",
    "            # Convert ISO 8601 duration (e.g., \"PT15M\") to minutes dynamically\n",
    "            df_fixed = df_filtered.withColumn(\n",
    "                \"interval_minutes\",\n",
    "                expr(\"CAST(SUBSTRING(resolution, 3, LENGTH(resolution) - 3) AS INT)\")  # Extracts \"15\" from \"PT15M\"\n",
    "            ).withColumn(\n",
    "                \"actual_time\",\n",
    "                expr(\"start_time + (position - 1) * interval_minutes * interval 1 minute\")\n",
    "            ).select(\n",
    "                col(\"actual_time\"),\n",
    "                col(\"quantity\")\n",
    "            )\n",
    "            \n",
    "            # Aggregate to hourly intervals\n",
    "            df_hourly = df_fixed.withColumn(\n",
    "                \"hourly_time\", date_trunc(\"hour\", col(\"actual_time\"))  # Round down to the hour\n",
    "            ).groupBy(\"hourly_time\").agg(\n",
    "                sum(\"quantity\").alias(\"hourly_quantity\")  # Sum all sub-hourly measurements\n",
    "            )\n",
    "\n",
    "            # Rename to match the structure of other time series\n",
    "            df_hourly = df_hourly.select(\n",
    "                col(\"hourly_time\").alias(\"timestamp\"), col(\"hourly_quantity\").alias(\"quantity\")\n",
    "            ).orderBy(\"timestamp\")\n",
    "            \n",
    "            print(f\"Read in {df_hourly.count()} values\")\n",
    "            \n",
    "            # Append DataFrame to list\n",
    "            dfs.append(df_hourly)\n",
    "\n",
    "# Merge all collected DataFrames\n",
    "if dfs:\n",
    "    Load = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        Load = Load.union(df)\n",
    "\n",
    "    Load.show(10)\n",
    "    Load.printSchema()\n",
    "else:\n",
    "    print(\"No data found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing II: Combine both Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-------------------+-------------------+------------------+-----------------+-------------------+------------------+-------------------+\n",
      "|summary|              load|       load_lag_24|           rainfall|     rainfall_lag_1|       temperature|         pressure|  sunshine_duration|        wind_speed|   wind_speed_lag_1|\n",
      "+-------+------------------+------------------+-------------------+-------------------+------------------+-----------------+-------------------+------------------+-------------------+\n",
      "|  count|             26280|             26280|              26280|              26280|             26280|            26280|              26280|             26280|              26280|\n",
      "|   mean|27150.157420091324|27146.609741248096|0.08296694568704333|0.08296694568704333|11.623370716295403|971.6488151146115|0.21699808588165032|1.0623818055673144| 1.0623803538258394|\n",
      "| stddev| 5199.763342043908| 5200.060296507467|0.18603561609247393| 0.1860356160924738| 8.281516682714477|7.878622799539257| 0.3195630964726697|0.2954983427867367|0.29549976002221084|\n",
      "|    min|           16055.0|           16055.0|                0.0|                0.0|-9.018181818181818|942.4799999999999|                0.0|0.2851789422336624| 0.2851789422336624|\n",
      "|    max|           41273.0|           41273.0|  2.359996000207385|  2.359996000207385| 33.85454545454546|994.9272727272727|                1.0|2.0719132752590443| 2.0719132752590443|\n",
      "+-------+------------------+------------------+-------------------+-------------------+------------------+-----------------+-------------------+------------------+-------------------+\n",
      "\n",
      "+-------------------+-------+-----------+--------+--------------+------------------+-----------------+-------------------+------------------+------------------+\n",
      "|               time|   load|load_lag_24|rainfall|rainfall_lag_1|       temperature|         pressure|  sunshine_duration|        wind_speed|  wind_speed_lag_1|\n",
      "+-------------------+-------+-----------+--------+--------------+------------------+-----------------+-------------------+------------------+------------------+\n",
      "|2022-01-02 00:00:00|21458.0|    22394.0|     0.0|           0.0| 2.972727272727272|980.6909090909091|                0.0|0.7548407495652851|0.7548407495652851|\n",
      "|2022-01-02 01:00:00|21151.0|    21804.0|     0.0|           0.0|2.5090909090909093|980.2636363636365|                0.0|0.6370577139089018|0.7548407495652851|\n",
      "|2022-01-02 02:00:00|20691.0|    20917.0|     0.0|           0.0|1.9909090909090912|979.8272727272725|                0.0|0.5725191927713306|0.6370577139089018|\n",
      "|2022-01-02 03:00:00|20715.0|    20705.0|     0.0|           0.0|1.7545454545454546|979.2727272727275|                0.0|0.6654956492294353|0.5725191927713306|\n",
      "|2022-01-02 04:00:00|21302.0|    20965.0|     0.0|           0.0|1.3363636363636362|            978.6|                0.0|0.5827233629455723|0.6654956492294353|\n",
      "|2022-01-02 05:00:00|22133.0|    21194.0|     0.0|           0.0|1.1363636363636365|978.1909090909089|                0.0|0.6747980418917487|0.5827233629455723|\n",
      "|2022-01-02 06:00:00|23505.0|    22118.0|     0.0|           0.0|0.8727272727272727|977.6636363636364|                0.0|0.5827233629455723|0.6747980418917487|\n",
      "|2022-01-02 07:00:00|25097.0|    23541.0|     0.0|           0.0|0.9818181818181819|977.4181818181819|                0.0|0.5776342934381009|0.5827233629455723|\n",
      "|2022-01-02 08:00:00|26442.0|    24966.0|     0.0|           0.0|1.0909090909090908|977.1727272727275|0.13636363636363635|0.5827233629455723|0.5776342934381009|\n",
      "|2022-01-02 09:00:00|27409.0|    25910.0|     0.0|           0.0|2.7636363636363637|976.6999999999999| 0.3727272727272728|0.6747980418917487|0.5827233629455723|\n",
      "+-------------------+-------+-----------+--------+--------------+------------------+-----------------+-------------------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- load: double (nullable = true)\n",
      " |-- load_lag_24: double (nullable = true)\n",
      " |-- rainfall: double (nullable = true)\n",
      " |-- rainfall_lag_1: double (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- sunshine_duration: double (nullable = true)\n",
      " |-- wind_speed: double (nullable = true)\n",
      " |-- wind_speed_lag_1: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if Load is not None and weather is not None:\n",
    "    # Join the data into a single DataFrame\n",
    "    data = Load.join(weather, Load.timestamp == weather.time, \"inner\").drop(\"time\")\n",
    "    \n",
    "    # Rename columns for better understanding\n",
    "    data = data.withColumnRenamed(\"timestamp\", \"time\")\n",
    "    data = data.withColumnRenamed(\"quantity\", \"load\")\n",
    "\n",
    "    data = data.withColumnRenamed(\"log_rr\", \"rainfall\")\n",
    "    data = data.withColumnRenamed(\"avg_tl\", \"temperature\")\n",
    "    data = data.withColumnRenamed(\"avg_p\", \"pressure\")\n",
    "    data = data.withColumnRenamed(\"avg_so_h\", \"sunshine_duration\")\n",
    "    data = data.withColumnRenamed(\"log_ff\", \"wind_speed\")\n",
    "    \n",
    "    # Add Lag features\n",
    "    \n",
    "    # Define window for lagging (ordered by time)\n",
    "    window_spec = Window().partitionBy().orderBy(\"time\")\n",
    "\n",
    "    # Add lag features (previous hour's values)\n",
    "    data = data.withColumn(\"rainfall_lag_1\", lag(\"rainfall\", 1).over(window_spec))\n",
    "    data = data.withColumn(\"wind_speed_lag_1\", lag(\"wind_speed\", 1).over(window_spec))\n",
    "    \n",
    "    # Add lag features (previous day's values)\n",
    "    data = data.withColumn(\"load_lag_24\", lag(\"load\", 24).over(window_spec))\n",
    "    \n",
    "    # Drop rows with missing values\n",
    "    data = data.dropna()\n",
    "    \n",
    "    # Reorder the columns\n",
    "    data = data.select(\"time\",\n",
    "                        \"load\", \n",
    "                        \"load_lag_24\",\n",
    "                        \"rainfall\", \n",
    "                        \"rainfall_lag_1\",\n",
    "                        \"temperature\", \n",
    "                        \"pressure\", \n",
    "                        \"sunshine_duration\", \n",
    "                        \"wind_speed\", \n",
    "                        \"wind_speed_lag_1\")\n",
    "    \n",
    "    # Print the schema and stats\n",
    "    data.describe().show()\n",
    "    data.show(10)\n",
    "    data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|load   |features                                                                                                                                                                       |\n",
      "+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|21458.0|[22394.0,0.0,0.0,2.972727272727272,980.6909090909091,0.0,0.7548407495652851,0.7548407495652851,0.034421692083641306,0.9994073979684656,0.0,1.0]                                |\n",
      "|21151.0|[21804.0,0.0,0.0,2.5090909090909093,980.2636363636365,0.0,0.6370577139089018,0.7548407495652851,0.034421692083641306,0.9994073979684656,0.25881963644308476,0.9659256678396477]|\n",
      "|20691.0|[20917.0,0.0,0.0,1.9909090909090912,979.8272727272725,0.0,0.5725191927713306,0.6370577139089018,0.034421692083641306,0.9994073979684656,0.5000010603626028,0.8660247915829389] |\n",
      "|20715.0|[20705.0,0.0,0.0,1.7545454545454546,979.2727272727275,0.0,0.6654956492294353,0.5725191927713306,0.034421692083641306,0.9994073979684656,0.7071080798594735,0.7071054825112363] |\n",
      "|21302.0|[20965.0,0.0,0.0,1.3363636363636362,978.6,0.0,0.5827233629455723,0.6654956492294353,0.034421692083641306,0.9994073979684656,0.8660266281835433,0.4999978792725455]             |\n",
      "+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Keep original timestamp before converting to Unix seconds\n",
    "unix_time_data = data.withColumn(\"unix_time\", unix_timestamp(\"time\"))  # Store Unix time separately\n",
    "\n",
    "# Extract day of the year (1-365/366) from the original timestamp column\n",
    "unix_time_data = unix_time_data.withColumn(\"day_of_year\", dayofyear(col(\"time\")))\n",
    "\n",
    "# Extract time of day in seconds (seconds since midnight)\n",
    "unix_time_data = unix_time_data.withColumn(\n",
    "    \"time_of_day\", expr(\"hour(time) * 3600 + minute(time) * 60 + second(time)\")\n",
    ")\n",
    "\n",
    "# Normalize day of the year to range (0, 2π) for periodic encoding\n",
    "ml_data = unix_time_data.withColumn(\"day_sin\", sin(2 * 3.1416 * col(\"day_of_year\") / 365))\n",
    "ml_data = ml_data.withColumn(\"day_cos\", cos(2 * 3.1416 * col(\"day_of_year\") / 365))\n",
    "\n",
    "# Normalize time of day\n",
    "ml_data = ml_data.withColumn(\"time_sin\", sin(2 * 3.1416 * col(\"time_of_day\") / 86400))\n",
    "ml_data = ml_data.withColumn(\"time_cos\", cos(2 * 3.1416 * col(\"time_of_day\") / 86400))\n",
    "\n",
    "# Define new feature columns\n",
    "feature_columns = [\"day_sin\", \"day_cos\", \"time_sin\", \"time_cos\", \"is_weekend\", \"rainfall\", \"temperature\", \"pressure\", \"sunshine_duration\", \"wind_speed\"]\n",
    "\n",
    "feature_columns = [ \"load_lag_24\",\n",
    "                    \"rainfall\", \n",
    "                    \"rainfall_lag_1\",\n",
    "                    \"temperature\", \n",
    "                    \"pressure\", \n",
    "                    \"sunshine_duration\", \n",
    "                    \"wind_speed\", \n",
    "                    \"wind_speed_lag_1\",\n",
    "                    \"day_sin\",\n",
    "                    \"day_cos\",\n",
    "                    \"time_sin\",\n",
    "                    \"time_cos\"\n",
    "                    ]\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "# Transform data\n",
    "df_ml = assembler.transform(ml_data).select(\"load\", \"features\")\n",
    "\n",
    "df_ml.cache()\n",
    "\n",
    "# Show transformed data\n",
    "df_ml.show(5, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|load   |features                                                                                                                                                                                 |\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|21458.0|[4.306488525727395,0.0,0.0,0.3589592808443017,124.47491573632132,0.0,2.5544669470788173,2.5544546956943335,0.04867875449108109,1.4133469539006789,0.0,1.4141850399325533]                |\n",
      "|21151.0|[4.193028302891851,0.0,0.0,0.30297480585023634,124.42068383080381,0.0,2.1558757585610926,2.5544546956943335,0.04867875449108109,1.4133469539006789,0.3660196941658062,1.3659976291456903]|\n",
      "|20691.0|[4.02245335771367,0.0,0.0,0.2404039220333397,124.3652980549561,0.0,1.9374700628508152,2.155865418845072,0.04867875449108109,1.4133469539006789,0.7070956350591399,1.2247193044672995]    |\n",
      "|20715.0|[3.981684599677847,0.0,0.0,0.21186281713440439,124.29491196481641,0.0,2.2521129660267785,1.9374607706222773,0.04867875449108109,1.4133469539006789,0.9999839528761927,0.99997799502168]  |\n",
      "|21302.0|[4.031684019910459,0.0,0.0,0.16136701615936497,124.20952556038455,0.0,1.9720021352746737,2.252102164751045,0.04867875449108109,1.4133469539006789,1.224724699962595,0.7070895208652368]  |\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "scaled_df = scaler.fit(df_ml).transform(df_ml).select(\"load\", \"features_scaled\")\n",
    "scaled_df = scaled_df.withColumnRenamed(\"features_scaled\", \"features\")\n",
    "scaled_df.cache()\n",
    "\n",
    "scaled_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(load=16055.0, features=DenseVector([3.438, 0.0, 0.0, 1.483, 123.2208, 0.0, 1.9513, 1.9513, 0.3257, -1.3762, 0.7071, 1.2247])),\n",
       " Row(load=16158.0, features=DenseVector([3.1484, 0.0, 0.0, 1.4732, 123.3464, 0.0, 2.0451, 1.8549, 0.7711, -1.1855, 0.366, 1.366])),\n",
       " Row(load=16168.0, features=DenseVector([3.2938, 0.0486, 0.192, 2.2778, 122.9414, 0.0, 3.7959, 3.9783, -0.6663, -1.2474, 0.366, 1.366])),\n",
       " Row(load=16209.0, features=DenseVector([3.449, 0.0, 0.0, 1.5588, 123.2157, 0.0, 1.9513, 2.1542, 0.3257, -1.3762, 0.366, 1.366])),\n",
       " Row(load=16216.0, features=DenseVector([3.1513, 0.0, 0.0, 1.4051, 123.3464, 0.0, 2.4621, 2.0451, 0.7711, -1.1855, 0.7071, 1.2247]))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly splits this :class:`DataFrame` with the provided weights.\n",
    "train_data, test_data = scaled_df.randomSplit([.8,.2],seed=1234)\n",
    "\n",
    "train_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2851.256628\n",
      "r2: 0.699920\n"
     ]
    }
   ],
   "source": [
    "# Linear regression.\n",
    "# The learning objective is to minimize the specified loss function, with regularization.\n",
    "#More about regParam and elasticNetParam (seen as penalties on the loss function that minimises error in predicitons) can be found here: https://runawayhorse001.github.io/LearningApacheSpark/reg.html\n",
    "# The maxIter parameter sets an upper limit on the number of iterations the optimization algorithm can perform regarding the loss function\n",
    "# By default the LinearRegression function expects another column named \"features\"\n",
    "\n",
    "lr = LinearRegression(labelCol=\"load\", maxIter=500)\n",
    "\n",
    "# Fits a model to the input dataset with optional parameters.\n",
    "linearModel = lr.fit(train_data)\n",
    "\n",
    "# Get the summary of the model\n",
    "trainingSummary = linearModel.summary\n",
    "\n",
    "# Print the Score of the model\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"load\")\n",
    "\n",
    "# Define hyperparameter grid\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.001, 0.01, 0.1, 1.0])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])  # 0 = Ridge, 1 = Lasso\n",
    "             .build())\n",
    "\n",
    "# Define evaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"load\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Define cross-validation\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Fit the model with cross-validation\n",
    "cv_model = cv.fit(train_data)\n",
    "\n",
    "# Get the best model\n",
    "best_model = cv_model.bestModel\n",
    "print(f\"Best regParam: {best_model._java_obj.getRegParam()}\")\n",
    "print(f\"Best elasticNetParam: {best_model._java_obj.getElasticNetParam()}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = best_model.transform(test_data)\n",
    "\n",
    "# Evaluate performance\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "# Show predictions\n",
    "predictions.select(\"load\", \"prediction\").show(10, truncate=False)\n",
    "\n",
    "r2 = evaluator.setMetricName(\"r2\").evaluate(predictions)\n",
    "print(f\"R2: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importance (Lasso Regression Coefficients):\n",
      "              Feature  Coefficient\n",
      "0         load_lag_24  3020.223594\n",
      "11           time_cos -1746.120104\n",
      "3         temperature  -635.724839\n",
      "5   sunshine_duration  -615.834222\n",
      "9             day_cos   321.831150\n",
      "1            rainfall   119.955653\n",
      "7    wind_speed_lag_1   -79.763799\n",
      "10           time_sin   -79.315728\n",
      "8             day_sin    60.714177\n",
      "6          wind_speed   -52.349136\n",
      "2      rainfall_lag_1    43.061944\n",
      "4            pressure    26.586304\n",
      "\n",
      "Features removed by Lasso: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get coefficients\n",
    "coefficients = best_model.coefficients.toArray()\n",
    "\n",
    "# Create a DataFrame for feature importance\n",
    "coef_df = pd.DataFrame({\"Feature\": feature_columns, \"Coefficient\": coefficients})\n",
    "\n",
    "# Sort by absolute coefficient values (fixing the issue)\n",
    "coef_df = coef_df.reindex(coef_df[\"Coefficient\"].abs().sort_values(ascending=False).index)\n",
    "\n",
    "# Display feature importance\n",
    "print(\"\\nFeature Importance (Lasso Regression Coefficients):\")\n",
    "print(coef_df)\n",
    "\n",
    "# Identify removed features (Lasso sets them to exactly 0)\n",
    "removed_features = coef_df[coef_df[\"Coefficient\"] == 0][\"Feature\"].tolist()\n",
    "print(f\"\\nFeatures removed by Lasso: {removed_features}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest RMSE: 2655.331627131015\n",
      "+-------+------------------+\n",
      "|load   |prediction        |\n",
      "+-------+------------------+\n",
      "|16120.0|19756.220040786684|\n",
      "|16170.0|19363.87555976215 |\n",
      "|16555.0|19883.084436726895|\n",
      "|16602.0|19352.431212810618|\n",
      "|16603.0|19653.89031174189 |\n",
      "|16634.0|19261.425831555174|\n",
      "|16643.0|19476.182713137518|\n",
      "|16746.0|19316.405360232406|\n",
      "|16833.0|19518.02307237193 |\n",
      "|16859.0|19405.34351009589 |\n",
      "+-------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Feature Importances (Random Forest):\n",
      "load_lag_24: 0.6383276555146126\n",
      "rainfall: 0.011059557408270014\n",
      "rainfall_lag_1: 0.010013318455556208\n",
      "temperature: 0.1612212502922963\n",
      "pressure: 0.03209565387580752\n",
      "sunshine_duration: 0.10765030750084678\n",
      "wind_speed: 0.021534098553229224\n",
      "wind_speed_lag_1: 0.01809815839938156\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Define the Random Forest model\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"load\", numTrees=100, maxDepth=10, seed=42)\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Evaluate performance\n",
    "evaluator = RegressionEvaluator(labelCol=\"load\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rf_rmse = evaluator.evaluate(rf_predictions)\n",
    "\n",
    "print(f\"Random Forest RMSE: {rf_rmse}\")\n",
    "\n",
    "# Show predictions\n",
    "rf_predictions.select(\"load\", \"prediction\").show(10, truncate=False)\n",
    "\n",
    "# Feature importance\n",
    "rf_feature_importance = rf_model.featureImportances\n",
    "print(\"\\nFeature Importances (Random Forest):\")\n",
    "for i, imp in enumerate(rf_feature_importance):\n",
    "    print(f\"{feature_columns[i]}: {imp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Regression RMSE: 2930.561547709331\n",
      "+-------+------------------+\n",
      "|load   |prediction        |\n",
      "+-------+------------------+\n",
      "|16120.0|19007.793203885783|\n",
      "|16170.0|19134.1688077703  |\n",
      "|16555.0|19794.277979894483|\n",
      "|16602.0|18408.548487121356|\n",
      "|16603.0|20398.044320557383|\n",
      "|16634.0|18177.71089276485 |\n",
      "|16643.0|19466.786875656457|\n",
      "|16746.0|18337.63203177735 |\n",
      "|16833.0|19769.41339639749 |\n",
      "|16859.0|19106.392148101062|\n",
      "+-------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PolynomialExpansion\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "\n",
    "# Define the Polynomial Expansion (degree = 2 for quadratic features)\n",
    "poly_expansion = PolynomialExpansion(degree=2, inputCol=\"features\", outputCol=\"poly_features\")\n",
    "\n",
    "# Apply transformation\n",
    "poly_data = poly_expansion.transform(train_data)\n",
    "\n",
    "# Define the Generalized Linear Model (GLM)\n",
    "glm = GeneralizedLinearRegression(featuresCol=\"poly_features\", labelCol=\"load\", family=\"gaussian\", link=\"identity\")\n",
    "\n",
    "# Train the model\n",
    "glm_model = glm.fit(poly_data)\n",
    "\n",
    "# Make predictions\n",
    "poly_predictions = glm_model.transform(poly_expansion.transform(test_data))\n",
    "\n",
    "# Evaluate performance\n",
    "evaluator = RegressionEvaluator(labelCol=\"load\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "poly_rmse = evaluator.evaluate(poly_predictions)\n",
    "\n",
    "print(f\"Polynomial Regression RMSE: {poly_rmse}\")\n",
    "\n",
    "# Show predictions\n",
    "poly_predictions.select(\"load\", \"prediction\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
