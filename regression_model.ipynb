{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harnessing Weather Insights for Accurate Energy Load Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: certifi==2024.12.14 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 7)) (2024.12.14)\n",
      "Requirement already satisfied: charset-normalizer==3.4.1 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 9)) (3.4.1)\n",
      "Requirement already satisfied: datetime==5.5 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 11)) (5.5)\n",
      "Requirement already satisfied: findspark==2.0.1 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 13)) (2.0.1)\n",
      "Requirement already satisfied: idna==3.10 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 15)) (3.10)\n",
      "Collecting numpy==2.2.2 (from -r requirements.txt (line 17))\n",
      "  Downloading numpy-2.2.2-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 19)) (0.10.9.7)\n",
      "Requirement already satisfied: pyspark==3.5.4 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 21)) (3.5.4)\n",
      "Requirement already satisfied: pytz==2024.2 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 23)) (2024.2)\n",
      "Requirement already satisfied: requests==2.32.3 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 25)) (2.32.3)\n",
      "Requirement already satisfied: urllib3==2.3.0 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 27)) (2.3.0)\n",
      "Requirement already satisfied: zope-interface==7.2 in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from -r requirements.txt (line 29)) (7.2)\n",
      "Requirement already satisfied: setuptools in p:\\development\\vs\\dp2-project\\.venv\\lib\\site-packages (from zope-interface==7.2->-r requirements.txt (line 29)) (75.8.0)\n",
      "Downloading numpy-2.2.2-cp313-cp313-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.3/12.6 MB 7.7 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.8/12.6 MB 5.0 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.9/12.6 MB 4.9 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 6.3/12.6 MB 8.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.1/12.6 MB 12.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 12.4 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-2.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windows OS detected\n"
     ]
    }
   ],
   "source": [
    "# Initialize via the full spark path\n",
    "if platform.system() == 'Windows':\n",
    "    print(\"Windows OS detected\")\n",
    "    findspark.init(\"C:/Spark/spark-3.5.4-bin-hadoop3\") # For my local machine\n",
    "else:\n",
    "    findspark.init(\"/usr/local/spark/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local\") \\\n",
    "      .appName(\"Linear Regression Model\") \\\n",
    "      .config(\"spark.executor.memory\", \"4gb\") \\\n",
    "      .config(\"spark.jars.packages\", \"com.databricks:spark-xml_2.12:0.15.0\") \\\n",
    "      .config(\"spark.executor.timeout\", \"300s\") \\\n",
    "      .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "      .getOrCreate()\n",
    "   \n",
    "# Main entry point for Spark functionality. A SparkContext represents the\n",
    "# connection to a Spark cluster, and can be used to create :class:`RDD` and\n",
    "# broadcast variables on that cluster.      \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing I: Read in Weather and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in data\\geosphere\\2022\\01.csv\n",
      "1490\n",
      "Reading in data\\geosphere\\2022\\02.csv\n",
      "1346\n",
      "Reading in data\\geosphere\\2022\\03.csv\n",
      "1490\n",
      "Reading in data\\geosphere\\2022\\04.csv\n",
      "1442\n",
      "Reading in data\\geosphere\\2022\\05.csv\n",
      "1490\n",
      "Reading in data\\geosphere\\2022\\06.csv\n",
      "1442\n",
      "Reading in data\\geosphere\\2022\\07.csv\n",
      "1490\n",
      "Reading in data\\geosphere\\2022\\08.csv\n",
      "1490\n",
      "Reading in data\\geosphere\\2022\\09.csv\n",
      "1442\n",
      "Reading in data\\geosphere\\2022\\10.csv\n",
      "1490\n",
      "Reading in data\\geosphere\\2022\\11.csv\n",
      "1442\n",
      "Reading in data\\geosphere\\2022\\12.csv\n",
      "1490\n",
      "Reading in data\\geosphere\\2023\\01.csv\n",
      "1490\n",
      "Reading in data\\geosphere\\2023\\02.csv\n",
      "1346\n",
      "Reading in data\\geosphere\\2023\\03.csv\n",
      "1490\n",
      "Reading in data\\geosphere\\2023\\04.csv\n",
      "1442\n",
      "Reading in data\\geosphere\\2023\\05.csv\n",
      "1490\n",
      "Reading in data\\geosphere\\2023\\06.csv\n",
      "1442\n",
      "Reading in data\\geosphere\\2023\\07.csv\n",
      "1490\n",
      "Reading in data\\geosphere\\2023\\08.csv\n",
      "1490\n",
      "Reading in data\\geosphere\\2023\\09.csv\n",
      "1442\n",
      "Reading in data\\geosphere\\2023\\10.csv\n",
      "1490\n",
      "Reading in data\\geosphere\\2023\\11.csv\n",
      "1442\n",
      "Reading in data\\geosphere\\2023\\12.csv\n",
      "1490\n",
      "Reading in data\\geosphere\\2024\\01.csv\n",
      "1490\n",
      "Reading in data\\geosphere\\2024\\02.csv\n",
      "1394\n",
      "Reading in data\\geosphere\\2024\\03.csv\n",
      "1490\n",
      "Reading in data\\geosphere\\2024\\04.csv\n",
      "1442\n",
      "Reading in data\\geosphere\\2024\\05.csv\n",
      "1490\n",
      "Reading in data\\geosphere\\2024\\06.csv\n",
      "1442\n",
      "Reading in data\\geosphere\\2024\\07.csv\n",
      "1490\n",
      "Reading in data\\geosphere\\2024\\08.csv\n",
      "1490\n",
      "Reading in data\\geosphere\\2024\\09.csv\n",
      "1442\n",
      "Reading in data\\geosphere\\2024\\10.csv\n",
      "1490\n",
      "Reading in data\\geosphere\\2024\\11.csv\n",
      "1442\n",
      "Reading in data\\geosphere\\2024\\12.csv\n",
      "1490\n",
      "+-------------------+------+------------------+-----------------+--------+------------------+\n",
      "|time               |avg_rr|avg_tl            |avg_p            |avg_so_h|avg_ff            |\n",
      "+-------------------+------+------------------+-----------------+--------+------------------+\n",
      "|2022-01-01 00:00:00|0.0   |13.15             |964.5999999999999|0.0     |7.65              |\n",
      "|2022-01-01 01:00:00|0.0   |12.75             |965.2            |0.0     |7.05              |\n",
      "|2022-01-01 02:00:00|0.0   |11.3              |965.75           |0.0     |5.3999999999999995|\n",
      "|2022-01-01 03:00:00|0.0   |11.100000000000001|965.75           |0.0     |3.5               |\n",
      "|2022-01-01 04:00:00|0.0   |11.95             |965.8            |0.0     |3.75              |\n",
      "|2022-01-01 05:00:00|0.0   |12.100000000000001|965.0999999999999|0.0     |4.5               |\n",
      "|2022-01-01 06:00:00|0.0   |12.3              |965.4            |0.0     |4.9               |\n",
      "|2022-01-01 07:00:00|0.0   |12.1              |966.0            |0.0     |4.35              |\n",
      "|2022-01-01 08:00:00|0.0   |12.35             |966.75           |0.55    |4.85              |\n",
      "|2022-01-01 09:00:00|0.0   |13.0              |967.1500000000001|0.75    |5.35              |\n",
      "+-------------------+------+------------------+-----------------+--------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "26305\n",
      "root\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- avg_rr: double (nullable = true)\n",
      " |-- avg_tl: double (nullable = true)\n",
      " |-- avg_p: double (nullable = true)\n",
      " |-- avg_so_h: double (nullable = true)\n",
      " |-- avg_ff: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in the data\n",
    "\n",
    "# Folder Structure\n",
    "# data\n",
    "# |-- geosphere\n",
    "# |   |-- YYYY\n",
    "# |      |-- MM.csv\n",
    "# |      |-- MM.csv\n",
    "# |\n",
    "# |-- transparency\n",
    "# |   |-- YYYY\n",
    "# |      |-- MM.xml\n",
    "# |      |-- MM.xml\n",
    "\n",
    "# Loop through the geosphere folder and read in the data\n",
    "\n",
    "# Define the base data folder\n",
    "base_path = Path(\"./data/geosphere\")\n",
    "\n",
    "# Collect all data frames first to optimize the union operation\n",
    "dfs = []\n",
    "\n",
    "for year_folder in base_path.iterdir():\n",
    "    if year_folder.is_dir():\n",
    "        for month_file in year_folder.glob(\"*.csv\"):\n",
    "            print(f\"Reading in {month_file}\")\n",
    "\n",
    "            df = spark.read.csv(str(month_file), header=True, inferSchema=True)\n",
    "\n",
    "            # Convert the time column (string) to a timestamp\n",
    "            df = df.withColumn(\"time\", to_timestamp(col(\"time\"), \"yyyy-MM-dd'T'HH:mmXXX\"))\n",
    "            \n",
    "            dfs.append(df)\n",
    "            \n",
    "            print(f\"Read in {df.count()} rows\")\n",
    "\n",
    "if dfs:\n",
    "    # Combine all DataFrames\n",
    "    weather = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        weather = weather.union(df)\n",
    "\n",
    "    # Aggregate measurements (average from different stations)\n",
    "    weather = (\n",
    "        weather.groupBy(\"time\")\n",
    "        .agg(\n",
    "            avg(\"rr\").alias(\"avg_rr\"),\n",
    "            avg(\"tl\").alias(\"avg_tl\"),\n",
    "            avg(\"p\").alias(\"avg_p\"),\n",
    "            avg(\"so_h\").alias(\"avg_so_h\"),\n",
    "            avg(\"ff\").alias(\"avg_ff\"),\n",
    "        )\n",
    "        .orderBy(\"time\")\n",
    "    )\n",
    "\n",
    "    weather.show(10, truncate=False)\n",
    "    \n",
    "    print(weather.count())\n",
    "    weather.printSchema()\n",
    "else:\n",
    "    print(\"No data found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading transparency data: data\\transparency\\2022\\01.xml\n",
      "Reading transparency data: data\\transparency\\2022\\02.xml\n",
      "Reading transparency data: data\\transparency\\2022\\03.xml\n",
      "Reading transparency data: data\\transparency\\2022\\04.xml\n",
      "Reading transparency data: data\\transparency\\2022\\05.xml\n",
      "Reading transparency data: data\\transparency\\2022\\06.xml\n",
      "Reading transparency data: data\\transparency\\2022\\07.xml\n",
      "Reading transparency data: data\\transparency\\2022\\08.xml\n",
      "Reading transparency data: data\\transparency\\2022\\09.xml\n",
      "Reading transparency data: data\\transparency\\2022\\10.xml\n",
      "Reading transparency data: data\\transparency\\2022\\11.xml\n",
      "Reading transparency data: data\\transparency\\2022\\12.xml\n",
      "Reading transparency data: data\\transparency\\2023\\01.xml\n",
      "Reading transparency data: data\\transparency\\2023\\02.xml\n",
      "Reading transparency data: data\\transparency\\2023\\03.xml\n",
      "Reading transparency data: data\\transparency\\2023\\04.xml\n",
      "Reading transparency data: data\\transparency\\2023\\05.xml\n",
      "Reading transparency data: data\\transparency\\2023\\06.xml\n",
      "Reading transparency data: data\\transparency\\2023\\07.xml\n",
      "Reading transparency data: data\\transparency\\2023\\08.xml\n",
      "Reading transparency data: data\\transparency\\2023\\09.xml\n",
      "Reading transparency data: data\\transparency\\2023\\10.xml\n",
      "Reading transparency data: data\\transparency\\2023\\11.xml\n",
      "Reading transparency data: data\\transparency\\2023\\12.xml\n",
      "Reading transparency data: data\\transparency\\2024\\01.xml\n",
      "Reading transparency data: data\\transparency\\2024\\02.xml\n",
      "Reading transparency data: data\\transparency\\2024\\03.xml\n",
      "Reading transparency data: data\\transparency\\2024\\04.xml\n",
      "Reading transparency data: data\\transparency\\2024\\05.xml\n",
      "Reading transparency data: data\\transparency\\2024\\06.xml\n",
      "Reading transparency data: data\\transparency\\2024\\07.xml\n",
      "Reading transparency data: data\\transparency\\2024\\08.xml\n",
      "Reading transparency data: data\\transparency\\2024\\09.xml\n",
      "Reading transparency data: data\\transparency\\2024\\10.xml\n",
      "Reading transparency data: data\\transparency\\2024\\11.xml\n",
      "Reading transparency data: data\\transparency\\2024\\12.xml\n",
      "+-------------------+--------+\n",
      "|          timestamp|quantity|\n",
      "+-------------------+--------+\n",
      "|2022-01-01 00:00:00| 22394.0|\n",
      "|2022-01-01 01:00:00| 21804.0|\n",
      "|2022-01-01 02:00:00| 20917.0|\n",
      "|2022-01-01 03:00:00| 20705.0|\n",
      "|2022-01-01 04:00:00| 20965.0|\n",
      "|2022-01-01 05:00:00| 21194.0|\n",
      "|2022-01-01 06:00:00| 22118.0|\n",
      "|2022-01-01 07:00:00| 23541.0|\n",
      "|2022-01-01 08:00:00| 24966.0|\n",
      "|2022-01-01 09:00:00| 25910.0|\n",
      "+-------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- quantity: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through the transparency folder and read in the energy data\n",
    "\n",
    "# Define base path for transparency data\n",
    "base_path = Path(\"./data/transparency\")\n",
    "\n",
    "# Collect DataFrames before performing union (optimization)\n",
    "dfs = []\n",
    "\n",
    "for year_folder in base_path.iterdir():\n",
    "    if year_folder.is_dir():\n",
    "        for month_file in year_folder.glob(\"*.xml\"):\n",
    "            print(f\"Reading transparency data: {month_file}\")\n",
    "\n",
    "            # Read XML data\n",
    "            df = spark.read.format('xml').option(\"rowTag\", \"GL_MarketDocument\").load(str(month_file))\n",
    "\n",
    "            # Extract and explode relevant fields\n",
    "            df_filtered = df.select(\n",
    "                col(\"TimeSeries.Period.timeInterval.start\").alias(\"start_time\"),\n",
    "                col(\"TimeSeries.Period.timeInterval.end\").alias(\"end_time\"),\n",
    "                col(\"TimeSeries.Period.resolution\").alias(\"resolution\"),\n",
    "                explode(col(\"TimeSeries.Period.Point\")).alias(\"Point\")  # Flatten Points\n",
    "            ).select(\n",
    "                col(\"start_time\"),\n",
    "                col(\"end_time\"),\n",
    "                col(\"resolution\"),\n",
    "                col(\"Point.position\").cast(\"int\").alias(\"position\"),\n",
    "                col(\"Point.quantity\").cast(\"double\").alias(\"quantity\")\n",
    "            )\n",
    "\n",
    "            # Convert ISO 8601 duration (e.g., \"PT15M\") to minutes dynamically\n",
    "            df_fixed = df_filtered.withColumn(\n",
    "                \"interval_minutes\",\n",
    "                expr(\"CAST(SUBSTRING(resolution, 3, LENGTH(resolution) - 3) AS INT)\")  # Extracts \"15\" from \"PT15M\"\n",
    "            ).withColumn(\n",
    "                \"actual_time\",\n",
    "                expr(\"start_time + (position - 1) * interval_minutes * interval 1 minute\")\n",
    "            ).select(\n",
    "                col(\"actual_time\"),\n",
    "                col(\"quantity\")\n",
    "            )\n",
    "            \n",
    "            # Aggregate to hourly intervals\n",
    "            df_hourly = df_fixed.withColumn(\n",
    "                \"hourly_time\", date_trunc(\"hour\", col(\"actual_time\"))  # Round down to the hour\n",
    "            ).groupBy(\"hourly_time\").agg(\n",
    "                sum(\"quantity\").alias(\"hourly_quantity\")  # Sum all sub-hourly measurements\n",
    "            )\n",
    "\n",
    "            # Rename to match the structure of other time series\n",
    "            df_hourly = df_hourly.select(\n",
    "                col(\"hourly_time\").alias(\"timestamp\"), col(\"hourly_quantity\").alias(\"quantity\")\n",
    "            ).orderBy(\"timestamp\")\n",
    "            \n",
    "            print(f\"Read in {df_hourly.count()} rows\")\n",
    "            \n",
    "            # Append DataFrame to list\n",
    "            dfs.append(df_hourly)\n",
    "\n",
    "# Merge all collected DataFrames\n",
    "if dfs:\n",
    "    Load = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        Load = Load.union(df)\n",
    "\n",
    "    Load.show(10)\n",
    "    Load.printSchema()\n",
    "else:\n",
    "    print(\"No data found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing II: Combine both Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+-----------------+-----------------+------------------+------------------+\n",
      "|summary|             load|           rainfall|      temperature|         pressure| sunshine_duration|        wind_speed|\n",
      "+-------+-----------------+-------------------+-----------------+-----------------+------------------+------------------+\n",
      "|  count|            26304|              26304|            26304|            26304|             26304|             26304|\n",
      "|   mean|27147.53615419708|0.09834435827250604|10.73671494829684|959.9205348996351|0.2120685066909976|  2.13615229622871|\n",
      "| stddev|5198.551548542349| 0.6008211250333231|  8.2578055043952|7.875779934448705|0.3395092943434029|1.3306188964835206|\n",
      "|    min|          16055.0|                0.0|            -11.0|            929.4|               0.0|              0.05|\n",
      "|    max|          41273.0|               46.3|             33.8|            982.8|               1.0|             10.35|\n",
      "+-------+-----------------+-------------------+-----------------+-----------------+------------------+------------------+\n",
      "\n",
      "+-------+-------------------+--------+------------------+-----------------+-----------------+------------------+\n",
      "|   load|               time|rainfall|       temperature|         pressure|sunshine_duration|        wind_speed|\n",
      "+-------+-------------------+--------+------------------+-----------------+-----------------+------------------+\n",
      "|22394.0|2022-01-01 00:00:00|     0.0|             13.15|964.5999999999999|              0.0|              7.65|\n",
      "|21804.0|2022-01-01 01:00:00|     0.0|             12.75|            965.2|              0.0|              7.05|\n",
      "|20917.0|2022-01-01 02:00:00|     0.0|              11.3|           965.75|              0.0|5.3999999999999995|\n",
      "|20705.0|2022-01-01 03:00:00|     0.0|11.100000000000001|           965.75|              0.0|               3.5|\n",
      "|20965.0|2022-01-01 04:00:00|     0.0|             11.95|            965.8|              0.0|              3.75|\n",
      "|21194.0|2022-01-01 05:00:00|     0.0|12.100000000000001|965.0999999999999|              0.0|               4.5|\n",
      "|22118.0|2022-01-01 06:00:00|     0.0|              12.3|            965.4|              0.0|               4.9|\n",
      "|23541.0|2022-01-01 07:00:00|     0.0|              12.1|            966.0|              0.0|              4.35|\n",
      "|24966.0|2022-01-01 08:00:00|     0.0|             12.35|           966.75|             0.55|              4.85|\n",
      "|25910.0|2022-01-01 09:00:00|     0.0|              13.0|967.1500000000001|             0.75|              5.35|\n",
      "+-------+-------------------+--------+------------------+-----------------+-----------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- load: double (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- rainfall: double (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- sunshine_duration: double (nullable = true)\n",
      " |-- wind_speed: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if Load is not None and weather is not None:\n",
    "    # Join the data into a single DataFrame\n",
    "    data = Load.join(weather, Load.timestamp == weather.time, \"inner\").drop(\"time\")\n",
    "    \n",
    "    # Rename columns for better understanding\n",
    "    data = data.withColumnRenamed(\"timestamp\", \"time\")\n",
    "    data = data.withColumnRenamed(\"quantity\", \"load\")\n",
    "\n",
    "    data = data.withColumnRenamed(\"avg_rr\", \"rainfall\")\n",
    "    data = data.withColumnRenamed(\"avg_tl\", \"temperature\")\n",
    "    data = data.withColumnRenamed(\"avg_p\", \"pressure\")\n",
    "    data = data.withColumnRenamed(\"avg_so_h\", \"sunshine_duration\")\n",
    "    data = data.withColumnRenamed(\"avg_ff\", \"wind_speed\")\n",
    "    \n",
    "    # Reorder the columns\n",
    "    data = data.select(\"load\", \"time\", \"rainfall\", \"temperature\", \"pressure\", \"sunshine_duration\", \"wind_speed\")\n",
    "    \n",
    "    # Print the schema and stats\n",
    "    data.describe().show()\n",
    "    data.show(10)\n",
    "    data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------------------------------------+\n",
      "|load   |features                                           |\n",
      "+-------+---------------------------------------------------+\n",
      "|22394.0|[1.0,0.0,0.0,13.15,964.5999999999999,0.0,7.65]     |\n",
      "|21804.0|[1.0,3600.0,0.0,12.75,965.2,0.0,7.05]              |\n",
      "|20917.0|[1.0,7200.0,0.0,11.3,965.75,0.0,5.3999999999999995]|\n",
      "|20705.0|[1.0,10800.0,0.0,11.100000000000001,965.75,0.0,3.5]|\n",
      "|20965.0|[1.0,14400.0,0.0,11.95,965.8,0.0,3.75]             |\n",
      "+-------+---------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Keep original timestamp before converting to Unix seconds\n",
    "unix_time_data = data.withColumn(\"unix_time\", unix_timestamp(\"time\"))  # Store Unix time separately\n",
    "\n",
    "# Extract day of the year (1-365/366) from the original timestamp column\n",
    "unix_time_data = unix_time_data.withColumn(\"day_of_year\", dayofyear(col(\"time\")))\n",
    "\n",
    "# Extract time of day in seconds (seconds since midnight)\n",
    "unix_time_data = unix_time_data.withColumn(\n",
    "    \"time_of_day\", expr(\"hour(time) * 3600 + minute(time) * 60 + second(time)\")\n",
    ")\n",
    "\n",
    "# Define features for ML model\n",
    "feature_columns = [\"day_of_year\", \"time_of_day\", \"rainfall\", \"temperature\", \"pressure\", \"sunshine_duration\", \"wind_speed\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "# Transform data\n",
    "df_ml = assembler.transform(unix_time_data).select(\"load\", \"features\")\n",
    "\n",
    "# Show transformed data\n",
    "df_ml.show(5, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------------------------------------------------------------------------------+\n",
      "|load   |features                                                                                                  |\n",
      "+-------+----------------------------------------------------------------------------------------------------------+\n",
      "|22394.0|[0.009481837858352278,0.0,0.0,1.5924327586791602,122.47675887702681,0.0,5.749204389188338]                |\n",
      "|21804.0|[0.009481837858352278,0.1444602776491528,0.0,1.5439937394037484,122.55294180811352,0.0,5.2982863978794486]|\n",
      "|20917.0|[0.009481837858352278,0.2889205552983056,0.0,1.368402294530381,122.62277616160964,0.0,4.0582619217800024] |\n",
      "|20705.0|[0.009481837858352278,0.4333808329474585,0.0,1.3441827848926753,122.62277616160964,0.0,2.630354949301854] |\n",
      "|20965.0|[0.009481837858352278,0.5778411105966113,0.0,1.447115700852925,122.6291247392002,0.0,2.8182374456805577]  |\n",
      "+-------+----------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "standard_scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "\n",
    "scalar = standard_scaler.fit(df_ml)\n",
    "\n",
    "scaled_df = scalar.transform(df_ml)\n",
    "\n",
    "scaled_df = scaled_df.select(\"load\",\"features_scaled\")\n",
    "scaled_df = scaled_df.withColumn(\"features\",col(\"features_scaled\"))\n",
    "scaled_df = scaled_df.select(\"load\",\"features\")\n",
    "\n",
    "scaled_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(load=20691.0, features=DenseVector([0.019, 0.2889, 0.0, 0.3572, 122.9339, 0.0, 0.4885])),\n",
       " Row(load=20715.0, features=DenseVector([0.019, 0.4334, 0.0, 0.3209, 122.8577, 0.0, 0.5636])),\n",
       " Row(load=20917.0, features=DenseVector([0.0095, 0.2889, 0.0, 1.3684, 122.6228, 0.0, 4.0583])),\n",
       " Row(load=21151.0, features=DenseVector([0.019, 0.1445, 0.0, 0.4481, 122.991, 0.0, 0.263])),\n",
       " Row(load=21194.0, features=DenseVector([0.0095, 0.7223, 0.0, 1.4653, 122.5402, 0.0, 3.3819]))]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly splits this :class:`DataFrame` with the provided weights.\n",
    "train_data, test_data = scaled_df.randomSplit([.8,.2],seed=1234)\n",
    "\n",
    "train_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression.\n",
    "# The learning objective is to minimize the specified loss function, with regularization.\n",
    "#More about regParam and elasticNetParam (seen as penalties on the loss function that minimises error in predicitons) can be found here: https://runawayhorse001.github.io/LearningApacheSpark/reg.html\n",
    "# The maxIter parameter sets an upper limit on the number of iterations the optimization algorithm can perform regarding the loss function\n",
    "# By default the LinearRegression function expects another column named \"features\"\n",
    "\n",
    "lr = LinearRegression(labelCol=\"load\", maxIter=10000, regParam=0, elasticNetParam=1)\n",
    "\n",
    "# Fits a model to the input dataset with optional parameters.\n",
    "linearModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- load: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n",
      "+-------+--------------------+------------------+\n",
      "|   load|            features|        prediction|\n",
      "+-------+--------------------+------------------+\n",
      "|20705.0|[0.00948183785835...|25297.115386954887|\n",
      "|20965.0|[0.00948183785835...|  25265.7506699917|\n",
      "|22671.0|[0.28445513575056...|28727.679859653035|\n",
      "|22927.0|[0.28445513575056...| 27370.51021741413|\n",
      "|22966.0|[0.01896367571670...|29107.485141034984|\n",
      "|23020.0|[0.21808227074210...|27611.210344646133|\n",
      "|23052.0|[0.04740918929176...| 26650.71034270929|\n",
      "|23334.0|[0.15170940573363...| 28207.05811014515|\n",
      "|23533.0|[0.21808227074210...|27775.351404601977|\n",
      "|23630.0|[0.08533654072517...|29194.864282486822|\n",
      "|23881.0|[0.08533654072517...|30205.325649301925|\n",
      "|23998.0|[0.07585470286681...|29416.121492942435|\n",
      "|24051.0|[0.21808227074210...|  28084.3036134808|\n",
      "|24135.0|[0.27497329789221...|28389.293179111868|\n",
      "|24250.0|[0.15170940573363...| 27864.36506334343|\n",
      "|24328.0|[0.15170940573363...|28802.985944709144|\n",
      "|24510.0|[0.20860043288375...|29471.888811196946|\n",
      "|24530.0|[0.14222756787528...|  26951.6098282062|\n",
      "|24629.0|[0.09481837858352...| 27937.71959050181|\n",
      "|24760.0|[0.02844551357505...|28866.317130341184|\n",
      "+-------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform  is used to perform value predictions using the trained model\n",
    "predicted = linearModel.transform(test_data)\n",
    "predicted.printSchema()\n",
    "\n",
    "predicted.select(\"load\",\"features\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 4536.848422\n",
      "r2: 0.242705\n",
      "MSE: 20582993.608009\n",
      "MAE: 3819.787288\n"
     ]
    }
   ],
   "source": [
    "# regression Summary\n",
    "trainingSummary = linearModel.summary\n",
    "\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)\n",
    "print(\"MSE: %f\" % trainingSummary.meanSquaredError)\n",
    "print(\"MAE: %f\" % trainingSummary.meanAbsoluteError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
