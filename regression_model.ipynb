{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harnessing Weather Insights for Accurate Energy Load Forecasting\n",
    "\n",
    "by Florian Schulze, Raffaela Länger, Johanna Kronfuß and Julian Janisch\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project uses PySpark to predict energy consumption based on weather data. First, weather data from a CSV file and energy consumption data from an XML file are loaded and transformed into a structured format. Next, comprehensive feature engineering is performed, including the creation of lag features, temporal encoding, and data standardization. The prepared data is then split into training and test sets to train two models: a linear regression and a random forest regressor. Model performance is evaluated using metrics such as RMSE and R², complemented by visual analyses like scatter plots and residual plots. Finally, the results are compared to determine the most effective model for predicting energy consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command installs all required dependencies listed in requirements.txt\n",
    "# It ensures that all necessary libraries are available for the project\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import important libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use several imports in this project to facilitate data processing, machine learning, and visualization. The platform module helps check system compatibility, while findspark ensures that PySpark is properly configured. The Pathlib library is useful for handling file paths across different operating systems. PySpark's SparkSession is essential for initializing a Spark environment, and the functions from pyspark.sql.functions and pyspark.sql.window support data manipulation and window functions. For machine learning, we use StandardScaler and VectorAssembler to preprocess features, while LinearRegression and RandomForestRegressor provide predictive modeling capabilities. The RegressionEvaluator helps assess model performance. Finally, matplotlib.pyplot is used to visualize data and results, making model evaluation more intuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System and environment setup  \n",
    "import platform  # Used to check system compatibility  \n",
    "import findspark  # Ensures PySpark is properly initialized  \n",
    "from pathlib import Path  # Facilitates file path management  \n",
    "\n",
    "# PySpark setup  \n",
    "from pyspark.sql import SparkSession  # Initializes the Spark session  \n",
    "from pyspark.sql.functions import *  # Provides SQL-like functions for data transformation  \n",
    "from pyspark.sql.window import Window  # Enables window functions for advanced aggregations  \n",
    "\n",
    "# PySpark ML features and models  \n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler  # Used for feature preprocessing  \n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor  # Machine learning models  \n",
    "from pyspark.ml.evaluation import RegressionEvaluator  # Evaluates model performance  \n",
    "\n",
    "# Visualization  \n",
    "import matplotlib.pyplot as plt  # Used for plotting and visualizing results  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code initializes PySpark based on the operating system. If the system runs on Windows, it sets up PySpark using a specified local installation path; otherwise, it defaults to a common Spark path for Linux/Mac environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PySpark based on the operating system\n",
    "\n",
    "# Check if the system is running Windows\n",
    "if platform.system() == 'Windows':  \n",
    "    print(\"Windows OS detected\")  # Print a message for clarity\n",
    "    # Initialize PySpark with the specific Spark installation path on Windows\n",
    "    findspark.init(\"C:/Spark/spark-3.5.4-bin-hadoop3\")  # Adjust this path based on your local setup\n",
    "else:  \n",
    "    # Initialize PySpark with the default path on Linux/Mac environments\n",
    "    findspark.init(\"/usr/local/spark/\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates a SparkSession, which is the entry point for using Spark functionality in the application. It configures the session with various parameters, such as memory allocation (32g), shuffle partitions (8), and package dependencies (for handling XML data), ensuring proper session behavior. The SparkSession.builder setup also configures timezone handling and heartbeat intervals, enabling smooth operation across distributed nodes. Finally, it retrieves the SparkContext object (sc), which provides access to low-level Spark functionalities, such as creating RDDs and broadcasting variables within the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\  # Runs the Spark application locally with all available cores\n",
    "    .appName(\"Linear Regression Model\") \\  # Sets the name of the Spark application\n",
    "    .config(\"spark.executor.memory\", \"32g\") \\  # Allocates 32 GB of memory to each Spark executor\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\  # Configures the number of partitions for shuffling data\n",
    "    .config(\"spark.jars.packages\", \"com.databricks:spark-xml_2.12:0.15.0\") \\  # Includes the XML handling package\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"100s\") \\  # Sets heartbeat interval to 100 seconds to monitor executors\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\") \\  # Configures the session to use UTC as the time zone\n",
    "    .getOrCreate()  # Initializes the SparkSession (creates if it doesn't exist, or gets the existing one)\n",
    "\n",
    "# Main entry point for Spark functionality. A SparkContext represents the\n",
    "# connection to a Spark cluster and is used to create RDDs and broadcast variables on the cluster.  \n",
    "sc = spark.sparkContext  # Retrieves the SparkContext associated with the SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing I: Read in Weather and Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code reads CSV files from a directory structure, processes them into a unified DataFrame, performs transformations, and aggregates weather data. Specifically, it calculates average measurements from different stations and applies log transformation to skewed data (rainfall and windspeed). The result is a cleaned and aggregated dataset ready for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in data/geosphere/2024/01.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2024/02.csv\n",
      "Read in 8364 rows\n",
      "Reading in data/geosphere/2024/03.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2024/04.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2024/05.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2024/06.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2024/07.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2024/08.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2024/09.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2024/10.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2024/11.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2024/12.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2022/01.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2022/02.csv\n",
      "Read in 8076 rows\n",
      "Reading in data/geosphere/2022/03.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2022/04.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2022/05.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2022/06.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2022/07.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2022/08.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2022/09.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2022/10.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2022/11.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2022/12.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2023/01.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2023/02.csv\n",
      "Read in 8076 rows\n",
      "Reading in data/geosphere/2023/03.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2023/04.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2023/05.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2023/06.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2023/07.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2023/08.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2023/09.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2023/10.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2023/11.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2023/12.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2015/01.csv\n",
      "Read in 7500 rows\n",
      "Reading in data/geosphere/2015/02.csv\n",
      "Read in 8076 rows\n",
      "Reading in data/geosphere/2015/03.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2015/04.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2015/05.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2015/06.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2015/07.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2015/08.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2015/09.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2015/10.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2015/11.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2015/12.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2016/01.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2016/02.csv\n",
      "Read in 8364 rows\n",
      "Reading in data/geosphere/2016/03.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2016/04.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2016/05.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2016/06.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2016/07.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2016/08.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2016/09.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2016/10.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2016/11.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2016/12.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2017/01.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2017/02.csv\n",
      "Read in 8076 rows\n",
      "Reading in data/geosphere/2017/03.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2017/04.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2017/05.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2017/06.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2017/07.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2017/08.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2017/09.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2017/10.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2017/11.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2017/12.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2018/01.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2018/02.csv\n",
      "Read in 8076 rows\n",
      "Reading in data/geosphere/2018/03.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2018/04.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2018/05.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2018/06.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2018/07.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2018/08.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2018/09.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2018/10.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2018/11.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2018/12.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2019/01.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2019/02.csv\n",
      "Read in 8076 rows\n",
      "Reading in data/geosphere/2019/03.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2019/04.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2019/05.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2019/06.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2019/07.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2019/08.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2019/09.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2019/10.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2019/11.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2019/12.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2020/01.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2020/02.csv\n",
      "Read in 8364 rows\n",
      "Reading in data/geosphere/2020/03.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2020/04.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2020/05.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2020/06.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2020/07.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2020/08.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2020/09.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2020/10.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2020/11.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2020/12.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2021/01.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2021/02.csv\n",
      "Read in 8076 rows\n",
      "Reading in data/geosphere/2021/03.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2021/04.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2021/05.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2021/06.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2021/07.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2021/08.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2021/09.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2021/10.csv\n",
      "Read in 8940 rows\n",
      "Reading in data/geosphere/2021/11.csv\n",
      "Read in 8652 rows\n",
      "Reading in data/geosphere/2021/12.csv\n",
      "Read in 8940 rows\n",
      "+-------------------+--------------------+--------------------+-----------------+--------------------+------------------+--------------------+------------------+\n",
      "|time               |avg_rr              |avg_tl              |avg_p            |avg_so_h            |avg_ff            |log_rr              |log_ff            |\n",
      "+-------------------+--------------------+--------------------+-----------------+--------------------+------------------+--------------------+------------------+\n",
      "|2015-01-06 00:00:00|0.09166666666666667 |-0.2916666666666665 |980.2909090909092|0.0                 |2.2583333333333333|0.08770558041910556 |1.1812158172033198|\n",
      "|2015-01-06 01:00:00|0.041666666666666664|-0.5166666666666666 |980.2090909090911|0.0                 |2.291666666666667 |0.040821994520255124|1.191394022119076 |\n",
      "|2015-01-06 02:00:00|0.008333333333333333|-0.35833333333333345|980.218181818182 |0.0                 |2.3083333333333336|0.008298802814695094|1.1964445379051445|\n",
      "|2015-01-06 03:00:00|0.0                 |-0.4416666666666669 |980.0909090909091|0.0                 |1.8499999999999999|0.0                 |1.047318994280559 |\n",
      "|2015-01-06 04:00:00|0.0                 |-0.7416666666666667 |979.9454545454547|0.0                 |1.9333333333333333|0.0                 |1.0761394328160512|\n",
      "|2015-01-06 05:00:00|0.0                 |-0.8333333333333335 |980.0090909090911|0.0                 |2.0166666666666666|0.0                 |1.104152469043725 |\n",
      "|2015-01-06 06:00:00|0.0                 |-0.9833333333333333 |980.0727272727273|0.0                 |1.7999999999999998|0.0                 |1.0296194171811581|\n",
      "|2015-01-06 07:00:00|0.0                 |-1.1083333333333334 |980.2909090909092|0.016666666666666666|1.55              |0.0                 |0.9360933591703348|\n",
      "|2015-01-06 08:00:00|0.0                 |-0.8166666666666668 |980.5636363636366|0.19166666666666665 |1.6500000000000001|0.0                 |0.9745596399981309|\n",
      "|2015-01-06 09:00:00|0.0                 |-0.08333333333333325|980.6454545454544|0.3                 |1.5416666666666667|0.0                 |0.9328200338253656|\n",
      "+-------------------+--------------------+--------------------+-----------------+--------------------+------------------+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "87553\n",
      "root\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- avg_rr: double (nullable = true)\n",
      " |-- avg_tl: double (nullable = true)\n",
      " |-- avg_p: double (nullable = true)\n",
      " |-- avg_so_h: double (nullable = true)\n",
      " |-- avg_ff: double (nullable = true)\n",
      " |-- log_rr: double (nullable = true)\n",
      " |-- log_ff: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in the data\n",
    "\n",
    "# Folder Structure\n",
    "# data\n",
    "# |-- geosphere\n",
    "# |   |-- YYYY\n",
    "# |      |-- MM.csv\n",
    "# |      |-- MM.csv\n",
    "# |\n",
    "# |-- transparency\n",
    "# |   |-- YYYY\n",
    "# |      |-- MM.xml\n",
    "# |      |-- MM.xml\n",
    "\n",
    "# Loop through the geosphere folder and read in the data\n",
    "\n",
    "from pyspark.sql.utils import AnalysisException  # Import for handling Spark-specific exceptions\n",
    "\n",
    "# Define the base data folder\n",
    "base_path = Path(\"./data/geosphere\")\n",
    "\n",
    "# Collect all data frames first to optimize the union operation\n",
    "dfs = []  # List to hold individual DataFrames\n",
    "\n",
    "# Error handling for directory and file iteration\n",
    "try:\n",
    "    for year_folder in base_path.iterdir():  # Iterate over the year folders\n",
    "        if year_folder.is_dir():  # Ensure we're only processing directories (e.g., YYYY)\n",
    "            for month_file in year_folder.glob(\"*.csv\"):  # Iterate over CSV files for each month\n",
    "                print(f\"Reading in {month_file}\")  # Print the file being read\n",
    "\n",
    "                try:\n",
    "                    # Read CSV file into DataFrame\n",
    "                    df = spark.read.csv(str(month_file), header=True, inferSchema=True)\n",
    "\n",
    "                    # Convert the time column (string) to a timestamp\n",
    "                    df = df.withColumn(\"time\", to_timestamp(col(\"time\"), \"yyyy-MM-dd'T'HH:mmXXX\"))\n",
    "\n",
    "                    dfs.append(df)  # Append the DataFrame to the list\n",
    "\n",
    "                    print(f\"Read in {df.count()} rows\")  # Print the number of rows read\n",
    "                except Exception as e:  # Handle any errors that may arise during file reading or processing\n",
    "                    print(f\"Error reading {month_file}: {e}\")\n",
    "\n",
    "except FileNotFoundError as e:  # Handle case if the base path is not found\n",
    "    print(f\"Base path not found: {e}\")\n",
    "except PermissionError as e:  # Handle permission issues\n",
    "    print(f\"Permission error: {e}\")\n",
    "except Exception as e:  # Catch any other unexpected errors during directory processing\n",
    "    print(f\"Unexpected error while processing directories: {e}\")\n",
    "\n",
    "# If DataFrames were successfully read in\n",
    "if dfs:  \n",
    "    try:\n",
    "        # Combine all DataFrames using union (a single large DataFrame)\n",
    "        weather = dfs[0]  # Start with the first DataFrame\n",
    "        for df in dfs[1:]:  # Loop over the rest of the DataFrames\n",
    "            weather = weather.union(df)  # Merge them together\n",
    "\n",
    "        # Aggregate measurements (average from different stations)\n",
    "        weather = (\n",
    "            weather.groupBy(\"time\")  # Group data by the 'time' column\n",
    "            .agg(\n",
    "                avg(\"rr\").alias(\"avg_rr\"),\n",
    "                avg(\"tl\").alias(\"avg_tl\"),\n",
    "                avg(\"p\").alias(\"avg_p\"),\n",
    "                avg(\"so_h\").alias(\"avg_so_h\"),\n",
    "                avg(\"ff\").alias(\"avg_ff\"),\n",
    "            )\n",
    "            .orderBy(\"time\")  # Order by time\n",
    "        )\n",
    "\n",
    "        # Log-transform the measurements (data for rainfall and windspeed is skewed)\n",
    "        weather = weather.withColumn(\"log_rr\", log1p(col(\"avg_rr\")))  # Log transform 'avg_rr'\n",
    "        weather = weather.withColumn(\"log_ff\", log1p(col(\"avg_ff\")))  # Log transform 'avg_ff'\n",
    "\n",
    "        # Drop original columns after transformation\n",
    "        weather = weather.drop(\"avg_rr\", \"avg_ff\")\n",
    "\n",
    "        # Display the first 10 rows of the processed DataFrame\n",
    "        weather.show(10, truncate=False)\n",
    "\n",
    "        # Print the number of rows in the final DataFrame\n",
    "        print(weather.count())\n",
    "\n",
    "        # Print the schema of the final DataFrame (structure)\n",
    "        weather.printSchema()\n",
    "    \n",
    "    except AnalysisException as e:  # Catch errors related to Spark SQL analysis (e.g., missing columns or invalid operations)\n",
    "        print(f\"Error during data processing (Spark analysis): {e}\")\n",
    "    except Exception as e:  # Catch other unexpected errors during transformation/aggregation\n",
    "        print(f\"Unexpected error during data processing: {e}\")\n",
    "else:\n",
    "    print(\"No data found\")  # If no DataFrames were read in, print an error message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script processes transparency energy data stored in XML format within a structured directory. It begins by defining the base path and iterating through yearly subdirectories, identifying and reading monthly XML files. Each file is parsed using Spark’s XML reader, extracting relevant timestamps, resolutions, and measured energy quantities. Since the data is structured with nested \"Point\" elements, the script flattens this hierarchy using explode(), ensuring that each measurement is represented as a separate row. To align timestamps, it converts ISO 8601 durations (e.g., \"PT15M\") into actual minutes and calculates the precise measurement time dynamically. The data is then aggregated into hourly intervals by summing up all measurements within each hour. Finally, the processed DataFrames are merged into a single dataset for further analysis. Throughout the process, robust error handling is implemented to catch issues related to missing files, permission restrictions, or incorrect data formats, ensuring smooth execution even in case of failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading transparency data: data/transparency/2024/01.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2024/02.xml\n",
      "Read in 696 values\n",
      "Reading transparency data: data/transparency/2024/03.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2024/04.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2024/05.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2024/06.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2024/07.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2024/08.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2024/09.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2024/10.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2024/11.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2024/12.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2022/01.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2022/02.xml\n",
      "Read in 672 values\n",
      "Reading transparency data: data/transparency/2022/03.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2022/04.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2022/05.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2022/06.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2022/07.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2022/08.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2022/09.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2022/10.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2022/11.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2022/12.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2023/01.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2023/02.xml\n",
      "Read in 672 values\n",
      "Reading transparency data: data/transparency/2023/03.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2023/04.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2023/05.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2023/06.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2023/07.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2023/08.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2023/09.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2023/10.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2023/11.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2023/12.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2015/01.xml\n",
      "Read in 624 values\n",
      "Reading transparency data: data/transparency/2015/02.xml\n",
      "Read in 672 values\n",
      "Reading transparency data: data/transparency/2015/03.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2015/04.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2015/05.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2015/06.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2015/07.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2015/08.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2015/09.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2015/10.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2015/11.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2015/12.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2016/01.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2016/02.xml\n",
      "Read in 696 values\n",
      "Reading transparency data: data/transparency/2016/03.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2016/04.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2016/05.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2016/06.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2016/07.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2016/08.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2016/09.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2016/10.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2016/11.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2016/12.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2017/01.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2017/02.xml\n",
      "Read in 672 values\n",
      "Reading transparency data: data/transparency/2017/03.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2017/04.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2017/05.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2017/06.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2017/07.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2017/08.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2017/09.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2017/10.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2017/11.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2017/12.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2018/01.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2018/02.xml\n",
      "Read in 672 values\n",
      "Reading transparency data: data/transparency/2018/03.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2018/04.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2018/05.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2018/06.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2018/07.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2018/08.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2018/09.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2018/10.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2018/11.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2018/12.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2019/01.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2019/02.xml\n",
      "Read in 672 values\n",
      "Reading transparency data: data/transparency/2019/03.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2019/04.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2019/05.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2019/06.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2019/07.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2019/08.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2019/09.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2019/10.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2019/11.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2019/12.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2020/01.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2020/02.xml\n",
      "Read in 696 values\n",
      "Reading transparency data: data/transparency/2020/03.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2020/04.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2020/05.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2020/06.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2020/07.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2020/08.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2020/09.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2020/10.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2020/11.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2020/12.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2021/01.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2021/02.xml\n",
      "Read in 672 values\n",
      "Reading transparency data: data/transparency/2021/03.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2021/04.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2021/05.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2021/06.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2021/07.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2021/08.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2021/09.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2021/10.xml\n",
      "Read in 744 values\n",
      "Reading transparency data: data/transparency/2021/11.xml\n",
      "Read in 720 values\n",
      "Reading transparency data: data/transparency/2021/12.xml\n",
      "Read in 744 values\n",
      "+-------------------+--------+\n",
      "|          timestamp|quantity|\n",
      "+-------------------+--------+\n",
      "|2024-01-01 00:00:00| 21923.0|\n",
      "|2024-01-01 01:00:00| 21307.0|\n",
      "|2024-01-01 02:00:00| 20442.0|\n",
      "|2024-01-01 03:00:00| 20285.0|\n",
      "|2024-01-01 04:00:00| 20641.0|\n",
      "|2024-01-01 05:00:00| 21190.0|\n",
      "|2024-01-01 06:00:00| 22396.0|\n",
      "|2024-01-01 07:00:00| 23710.0|\n",
      "|2024-01-01 08:00:00| 24667.0|\n",
      "|2024-01-01 09:00:00| 25113.0|\n",
      "+-------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- quantity: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through the transparency folder and read in the energy data\n",
    "\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.functions import expr, col, explode, date_trunc, sum\n",
    "\n",
    "# Define the base path for transparency data (directory containing the XML files)\n",
    "base_path = Path(\"./data/transparency\")\n",
    "\n",
    "# List to hold DataFrames before combining them (optimization for union operation)\n",
    "dfs = []  \n",
    "\n",
    "# Error handling for the iteration over the directory\n",
    "try:\n",
    "    # Loop through each year folder in the base directory\n",
    "    for year_folder in base_path.iterdir():  # Iterate over the year folders\n",
    "        if year_folder.is_dir():  # Ensure we're processing directories (e.g., \"YYYY\")\n",
    "            # Iterate over all XML files in each year folder (e.g., \"MM.xml\")\n",
    "            for month_file in year_folder.glob(\"*.xml\"):  # Iterate over XML files for each month\n",
    "                print(f\"Reading transparency data: {month_file}\")  # Print the file being processed\n",
    "\n",
    "                try:\n",
    "                    # Read the XML file into a Spark DataFrame\n",
    "                    # The 'rowTag' option helps Spark to understand the structure of the XML document\n",
    "                    df = spark.read.format('xml').option(\"rowTag\", \"GL_MarketDocument\").load(str(month_file))\n",
    "\n",
    "                    # Extract and explode relevant fields from the XML\n",
    "                    # The `explode` function flattens the nested `Point` structure into individual rows\n",
    "                    df_filtered = df.select(\n",
    "                        col(\"TimeSeries.Period.timeInterval.start\").alias(\"start_time\"),\n",
    "                        col(\"TimeSeries.Period.timeInterval.end\").alias(\"end_time\"),\n",
    "                        col(\"TimeSeries.Period.resolution\").alias(\"resolution\"),\n",
    "                        explode(col(\"TimeSeries.Period.Point\")).alias(\"Point\")  # Flatten the 'Point' structure\n",
    "                    ).select(\n",
    "                        col(\"start_time\"),\n",
    "                        col(\"end_time\"),\n",
    "                        col(\"resolution\"),\n",
    "                        col(\"Point.position\").cast(\"int\").alias(\"position\"),  # Cast position to integer\n",
    "                        col(\"Point.quantity\").cast(\"double\").alias(\"quantity\")  # Cast quantity to double\n",
    "                    )\n",
    "\n",
    "                    # Convert ISO 8601 duration (e.g., \"PT15M\") to minutes dynamically\n",
    "                    df_fixed = df_filtered.withColumn(\n",
    "                        \"interval_minutes\",\n",
    "                        expr(\"CAST(SUBSTRING(resolution, 3, LENGTH(resolution) - 3) AS INT)\")  # Extracts \"15\" from \"PT15M\"\n",
    "                    ).withColumn(\n",
    "                        \"actual_time\",\n",
    "                        expr(\"start_time + (position - 1) * interval_minutes * interval 1 minute\")  # Calculate actual time\n",
    "                    ).select(\n",
    "                        col(\"actual_time\"),  # Select the calculated 'actual_time'\n",
    "                        col(\"quantity\")  # Select the 'quantity' from the 'Point'\n",
    "                    )\n",
    "\n",
    "                    # Aggregate the data to hourly intervals by truncating time to the hour\n",
    "                    # This step ensures that we get sums of the quantity over each hour\n",
    "                    df_hourly = df_fixed.withColumn(\n",
    "                        \"hourly_time\", date_trunc(\"hour\", col(\"actual_time\"))  # Round down the time to the nearest hour\n",
    "                    ).groupBy(\"hourly_time\").agg(\n",
    "                        sum(\"quantity\").alias(\"hourly_quantity\")  # Aggregate by summing the quantities\n",
    "                    )\n",
    "\n",
    "                    # Rename the columns to match the desired output format and order by timestamp\n",
    "                    df_hourly = df_hourly.select(\n",
    "                        col(\"hourly_time\").alias(\"timestamp\"),  # Rename 'hourly_time' to 'timestamp'\n",
    "                        col(\"hourly_quantity\").alias(\"quantity\")  # Rename 'hourly_quantity' to 'quantity'\n",
    "                    ).orderBy(\"timestamp\")  # Order by timestamp (hourly)\n",
    "\n",
    "                    print(f\"Read in {df_hourly.count()} values\")  # Print the number of rows read from the file\n",
    "\n",
    "                    # Append the DataFrame to the list for later merging\n",
    "                    dfs.append(df_hourly)\n",
    "                \n",
    "                except Exception as e:  # Catch any errors during reading or processing the XML file\n",
    "                    print(f\"Error processing file {month_file}: {e}\")  # Print the error message\n",
    "\n",
    "except FileNotFoundError as e:  # Handle case if the base directory is not found\n",
    "    print(f\"Base path not found: {e}\")\n",
    "except PermissionError as e:  # Handle any permission issues when accessing files\n",
    "    print(f\"Permission error: {e}\")\n",
    "except Exception as e:  # Catch any other unexpected errors during the iteration process\n",
    "    print(f\"Unexpected error while processing directories: {e}\")\n",
    "\n",
    "# If any DataFrames were successfully read and processed\n",
    "if dfs:\n",
    "    try:\n",
    "        # Merge all collected DataFrames into a single DataFrame\n",
    "        Load = dfs[0]  # Start with the first DataFrame\n",
    "        for df in dfs[1:]:  # Loop over the remaining DataFrames\n",
    "            Load = Load.union(df)  # Union them together (combine them)\n",
    "\n",
    "        Load.show(10)  # Show the first 10 rows of the final DataFrame\n",
    "        Load.printSchema()  # Print the schema (structure) of the final DataFrame\n",
    "\n",
    "    except AnalysisException as e:  # Catch any errors related to Spark SQL during the merge (e.g., schema issues)\n",
    "        print(f\"Error during data merging (Spark analysis): {e}\")\n",
    "    except Exception as e:  # Catch other unexpected errors during the merging process\n",
    "        print(f\"Unexpected error during data merging: {e}\")\n",
    "else:\n",
    "    print(\"No data found.\")  # If no DataFrames were read, print an error message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing II: Combine both Data Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code combines the two previously created DataFrames, Load (energy consumption data) and weather (weather data), into a single DataFrame called data. The merging is done using an inner join on the common time column (timestamp in Load and time in weather). After the join, several columns are renamed for better readability, such as renaming log_rr to rainfall and avg_tl to temperature.\n",
    "\n",
    "Next, lag features are added to incorporate past values as predictors for future forecasts. A Spark window specification (Window().partitionBy().orderBy(\"time\")) is defined to order the data by time. The code adds lag values for the previous hour (rainfall_lag_1 and wind_speed_lag_1) and for the same hour on the past seven days (load_lag_24h_prev_day, load_lag_48h_prev_day, ..., load_lag_168h_prev_day).\n",
    "\n",
    "After computing the lag features, the code removes any rows with missing values to ensure data integrity. Finally, the columns are reordered, and the schema, descriptive statistics, and a sample of the data are displayed for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+------------------+-------------------+-----------------+-------------------+-------------------+------------------+---------------------+---------------------+---------------------+---------------------+----------------------+----------------------+----------------------+\n",
      "|summary|              load|           rainfall|    rainfall_lag_1|        temperature|         pressure|  sunshine_duration|         wind_speed|  wind_speed_lag_1|load_lag_24h_prev_day|load_lag_48h_prev_day|load_lag_72h_prev_day|load_lag_96h_prev_day|load_lag_120h_prev_day|load_lag_144h_prev_day|load_lag_168h_prev_day|\n",
      "+-------+------------------+-------------------+------------------+-------------------+-----------------+-------------------+-------------------+------------------+---------------------+---------------------+---------------------+---------------------+----------------------+----------------------+----------------------+\n",
      "|  count|             87384|              87384|             87384|              87384|            87384|              87384|              87384|             87384|                87384|                87384|                87384|                87384|                 87384|                 87384|                 87384|\n",
      "|   mean|28057.059747779913| 0.0805927352665501|0.0805927352665501| 11.019483247401201|971.7633847816335| 0.2236365283569004| 1.0342648521471045| 1.034266339178587|   28057.199098233086|   28055.684084042845|    28054.91435503067|   28054.586434587567|    28055.197827977663|    28055.984264853978|    28055.494289572463|\n",
      "| stddev|  5452.92164404097|0.18004157905497123|0.1800415790549711|  8.393033219944751|7.681132332378645|0.32291062218707056|0.27776032186524446|0.2777593779952798|    5452.868346376266|   5453.4302825756895|   5453.9635805241605|     5454.00065839318|      5454.04345710899|     5454.276604501356|     5454.660288519905|\n",
      "|    min|            2653.0|                0.0|               0.0|-13.991666666666667|933.3545454545455|                0.0| 0.2851789422336624|0.2851789422336624|               2653.0|               2653.0|               2653.0|               2653.0|                2653.0|                2653.0|                2653.0|\n",
      "|    max|           43211.0|  2.359996000207385| 2.359996000207385|  34.81666666666667|998.3727272727273|                1.0| 2.0719132752590443|2.0719132752590443|              43211.0|              43211.0|              43211.0|              43211.0|               43211.0|               43211.0|               43211.0|\n",
      "+-------+------------------+-------------------+------------------+-------------------+-----------------+-------------------+-------------------+------------------+---------------------+---------------------+---------------------+---------------------+----------------------+----------------------+----------------------+\n",
      "\n",
      "+-------------------+-------+--------+--------------+--------------------+-----------------+-------------------+------------------+------------------+---------------------+---------------------+---------------------+---------------------+----------------------+----------------------+----------------------+\n",
      "|               time|   load|rainfall|rainfall_lag_1|         temperature|         pressure|  sunshine_duration|        wind_speed|  wind_speed_lag_1|load_lag_24h_prev_day|load_lag_48h_prev_day|load_lag_72h_prev_day|load_lag_96h_prev_day|load_lag_120h_prev_day|load_lag_144h_prev_day|load_lag_168h_prev_day|\n",
      "+-------------------+-------+--------+--------------+--------------------+-----------------+-------------------+------------------+------------------+---------------------+---------------------+---------------------+---------------------+----------------------+----------------------+----------------------+\n",
      "|2015-01-13 00:00:00|26782.0|     0.0|           0.0|               0.875|977.0727272727272|                0.0|1.0355541527009728|0.9229352745928237|              25661.0|              20349.0|              22633.0|              26239.0|               25428.0|               22272.0|               20881.0|\n",
      "|2015-01-13 01:00:00|26012.0|     0.0|           0.0|  0.6583333333333333|            977.1|                0.0|0.9458495341156996|1.0355541527009728|              24350.0|              19702.0|              19743.0|              25872.0|               25441.0|               22567.0|               20059.0|\n",
      "|2015-01-13 02:00:00|24886.0|     0.0|           0.0|  0.5083333333333334|977.0454545454545|                0.0|1.0560526742493137|0.9458495341156996|              24066.0|              19207.0|              20472.0|              26102.0|               25669.0|               22190.0|               19471.0|\n",
      "|2015-01-13 03:00:00|25195.0|     0.0|           0.0|  0.1416666666666667|976.8090909090909|                0.0|0.9776993600027986|1.0560526742493137|              23856.0|              19160.0|              21409.0|              25593.0|               25206.0|               23385.0|               19894.0|\n",
      "|2015-01-13 04:00:00|27548.0|     0.0|           0.0| 0.09999999999999987|976.6727272727272|                0.0|1.0116009116784799|0.9776993600027986|              25154.0|              19808.0|              21972.0|              27909.0|               26404.0|               24538.0|               19704.0|\n",
      "|2015-01-13 05:00:00|30555.0|     0.0|           0.0|               0.125|976.5363636363636|                0.0|0.9129518306086406|1.0116009116784799|              27075.0|              20851.0|              22601.0|              28024.0|               28772.0|               26633.0|               18966.0|\n",
      "|2015-01-13 06:00:00|34149.0|     0.0|           0.0| -0.6250000000000001|976.2818181818183|                0.0|0.8649974374866045|0.9129518306086406|              29311.0|              21477.0|              25539.0|              27545.0|               30746.0|               30227.0|               21746.0|\n",
      "|2015-01-13 07:00:00|35502.0|     0.0|           0.0| -1.0166666666666666|976.3363636363637|                0.0|0.6196800286780728|0.8649974374866045|              29592.0|              23052.0|              27210.0|              27789.0|               32244.0|               29202.0|               23284.0|\n",
      "|2015-01-13 08:00:00|35342.0|     0.0|           0.0|-0.23333333333333342|976.3909090909092|0.31666666666666665| 0.672093771362113|0.6196800286780728|              30076.0|              22372.0|              27854.0|              27001.0|               33156.0|               30357.0|               24994.0|\n",
      "|2015-01-13 09:00:00|35440.0|     0.0|           0.0|  2.3333333333333335|976.3000000000001|               0.85|0.6635467107836545| 0.672093771362113|              32653.0|              23937.0|              26867.0|              25962.0|               30648.0|               32124.0|               26433.0|\n",
      "+-------------------+-------+--------+--------------+--------------------+-----------------+-------------------+------------------+------------------+---------------------+---------------------+---------------------+---------------------+----------------------+----------------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- load: double (nullable = true)\n",
      " |-- rainfall: double (nullable = true)\n",
      " |-- rainfall_lag_1: double (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- sunshine_duration: double (nullable = true)\n",
      " |-- wind_speed: double (nullable = true)\n",
      " |-- wind_speed_lag_1: double (nullable = true)\n",
      " |-- load_lag_24h_prev_day: double (nullable = true)\n",
      " |-- load_lag_48h_prev_day: double (nullable = true)\n",
      " |-- load_lag_72h_prev_day: double (nullable = true)\n",
      " |-- load_lag_96h_prev_day: double (nullable = true)\n",
      " |-- load_lag_120h_prev_day: double (nullable = true)\n",
      " |-- load_lag_144h_prev_day: double (nullable = true)\n",
      " |-- load_lag_168h_prev_day: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if Load is not None and weather is not None:    \n",
    "    # Join the two datasets on the timestamp column\n",
    "    # This ensures that energy load data (Load) and weather data (weather) are aligned in time\n",
    "    data = Load.join(weather, Load.timestamp == weather.time, \"inner\").drop(\"time\")\n",
    "    \n",
    "    # Rename columns for better readability and consistency\n",
    "    data = data.withColumnRenamed(\"timestamp\", \"time\")  # Standardize time column name\n",
    "    data = data.withColumnRenamed(\"quantity\", \"load\")  # Rename energy load column\n",
    "\n",
    "    # Rename weather-related columns for clarity\n",
    "    data = data.withColumnRenamed(\"log_rr\", \"rainfall\")  # Log-transformed rainfall\n",
    "    data = data.withColumnRenamed(\"avg_tl\", \"temperature\")  # Average temperature\n",
    "    data = data.withColumnRenamed(\"avg_p\", \"pressure\")  # Atmospheric pressure\n",
    "    data = data.withColumnRenamed(\"avg_so_h\", \"sunshine_duration\")  # Sunshine duration\n",
    "    data = data.withColumnRenamed(\"log_ff\", \"wind_speed\")  # Log-transformed wind speed\n",
    "\n",
    "    # Adding Lag Features\n",
    "    \n",
    "    # Define a window specification for lagging, ordered by time\n",
    "    # This ensures that lag values are calculated correctly based on previous timestamps\n",
    "    window_spec = Window().partitionBy().orderBy(\"time\")\n",
    "\n",
    "    # Short-term lag features (previous hour)\n",
    "    data = data.withColumn(\"rainfall_lag_1\", lag(\"rainfall\", 1).over(window_spec))  # Rainfall 1 hour ago\n",
    "    data = data.withColumn(\"wind_speed_lag_1\", lag(\"wind_speed\", 1).over(window_spec))  # Wind speed 1 hour ago\n",
    "    \n",
    "    # Long-term lag features (values from previous days at the same hour)\n",
    "    days = []\n",
    "    for i in range(24, 24 * 7 + 1, 24):  # Lagging in 24-hour steps up to 7 days\n",
    "        data = data.withColumn(f\"load_lag_{i}h_prev_day\", lag(\"load\", i).over(window_spec))  # Load at the same hour from past days\n",
    "        days.append(f\"load_lag_{i}h_prev_day\")  # Store column names for ordering\n",
    "    \n",
    "    # Drop rows with missing values resulting from the lagging process\n",
    "    # This prevents issues with model training or analysis due to NaNs\n",
    "    data = data.dropna()\n",
    " \n",
    "    # Final Data Processing\n",
    "    \n",
    "    # Reorder columns for better structure\n",
    "    data = data.select(\n",
    "        \"time\",  # Timestamp\n",
    "        \"load\",  # Energy load\n",
    "        \"rainfall\", \"rainfall_lag_1\",  # Rainfall and its lag\n",
    "        \"temperature\", \"pressure\", \"sunshine_duration\",  # Weather conditions\n",
    "        \"wind_speed\", \"wind_speed_lag_1\",  # Wind speed and its lag\n",
    "        *days  # Add all long-term lag features\n",
    "    )\n",
    "    \n",
    "    # Print basic statistics and data validation\n",
    "    data.describe().show()  # Summary statistics (mean, std, min, max)\n",
    "    data.show(10)  # Show first 10 rows of the processed dataset\n",
    "    data.printSchema()  # Display column names and data types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code prepares the dataset for machine learning by engineering new features and structuring the data in a format suitable for training. First, it extracts time-based features from the timestamp, such as the day of the year and time of day in seconds, which help capture seasonal and daily variations in energy load. To handle periodic patterns, these time-based features are transformed using cosine encoding, ensuring smooth cyclic transitions. Next, the script defines the feature set, including weather conditions (rainfall, temperature, pressure, sunshine, and wind speed), as well as lag features from previous hours and days. These lag features help the model understand past trends and how they influence the present load. Finally, the selected features are assembled into a feature vector using VectorAssembler, creating a structured format for machine learning. The transformed dataset is then cached to optimize performance during model training, and a preview of the processed data is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8a40136883ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Show transformed data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mdf_ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Keep the original timestamp before converting it to Unix time (seconds since epoch)\n",
    "# This helps retain the date-time information while adding machine-learning-friendly features\n",
    "unix_time_data = data.withColumn(\"unix_time\", unix_timestamp(\"time\"))\n",
    "\n",
    "# Extract the day of the year (1-365/366) from the timestamp column\n",
    "# This helps model seasonal patterns across the year\n",
    "unix_time_data = unix_time_data.withColumn(\"day_of_year\", dayofyear(col(\"time\")))\n",
    "\n",
    "# Extract the time of day in seconds (seconds since midnight)\n",
    "# This allows the model to capture daily variations in energy load\n",
    "unix_time_data = unix_time_data.withColumn(\n",
    "    \"time_of_day\", expr(\"hour(time) * 3600 + minute(time) * 60 + second(time)\")\n",
    ")\n",
    "\n",
    "# Normalize the day of the year to a range of (0, 2π) using a cosine transformation\n",
    "# This encodes the cyclical nature of the year while maintaining periodic continuity\n",
    "ml_data = unix_time_data.withColumn(\"day_cos\", cos(2 * 3.1416 * col(\"day_of_year\") / 365))\n",
    "\n",
    "# Normalize the time of day in a similar way\n",
    "# This ensures the model understands daily cycles (e.g., midnight is close to 23:59)\n",
    "ml_data = ml_data.withColumn(\"time_cos\", cos(2 * 3.1416 * col(\"time_of_day\") / 86400))\n",
    "\n",
    "# Define the features used for machine learning\n",
    "feature_columns = [\n",
    "    \"day_cos\",  # Encoded day of the year\n",
    "    \"time_cos\",  # Encoded time of the day\n",
    "    \"rainfall\",  \n",
    "    \"rainfall_lag_1\",\n",
    "    \"temperature\", \n",
    "    \"pressure\", \n",
    "    \"sunshine_duration\", \n",
    "    \"wind_speed\", \n",
    "    \"wind_speed_lag_1\", \n",
    "    *days  # Long-term lag features (previous days' load)\n",
    "]\n",
    "\n",
    "# Assemble the selected features into a single vector column for ML processing\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "# Transform the dataset to generate the feature vector\n",
    "df_ml = assembler.transform(ml_data).select(\"load\", \"features\")\n",
    "\n",
    "# Cache the transformed dataset for faster access during model training\n",
    "df_ml.cache()\n",
    "\n",
    "# Show the first 5 rows of the transformed dataset\n",
    "df_ml.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scaling with Standardization\n",
    "\n",
    "This step standardizes the dataset's features to improve the stability and performance of machine learning models. Since raw data can have vastly different numerical ranges (e.g., temperature in degrees vs. wind speed in m/s), scaling ensures that all features contribute equally to the model.\n",
    "\n",
    "Using StandardScaler, the features are normalized by removing the mean and scaling to unit variance, which helps prevent models from being biased toward large-magnitude features. The transformation is then applied to the dataset, and the scaled features are stored under a new column name (features_scaled). To maintain consistency, the column is renamed back to \"features\" before caching the dataset for optimized performance.\n",
    "\n",
    "Finally, a preview of the first five rows is displayed to confirm that the transformation was applied correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a StandardScaler to normalize the feature values\n",
    "# StandardScaler standardizes each feature by removing the mean and scaling to unit variance\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "\n",
    "# Fit the scaler on the dataset and transform it\n",
    "# This ensures that all features have a comparable scale, improving model performance\n",
    "scaled_df = scaler.fit(df_ml).transform(df_ml).select(\"load\", \"features_scaled\")\n",
    "\n",
    "# Rename the scaled features column back to \"features\" for consistency\n",
    "scaled_df = scaled_df.withColumnRenamed(\"features_scaled\", \"features\")\n",
    "\n",
    "# Cache the scaled dataset to improve performance in subsequent operations\n",
    "scaled_df.cache()\n",
    "\n",
    "# Show the first 5 rows of the transformed dataset with scaled features\n",
    "scaled_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Splitting: Training and Testing Set\n",
    "\n",
    "The dataset is randomly split into two subsets: one for training and one for testing. This is crucial for evaluating model performance and ensuring that the model generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly splits the DataFrame into training (80%) and testing (20%) sets using the specified seed value\n",
    "train_data, test_data = scaled_df.randomSplit([.8, .2], seed=1234)\n",
    "\n",
    "# Display the first 5 rows of the training data to confirm the split\n",
    "train_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression\n",
    "\n",
    "This step trains a linear regression model on the training dataset, using the features and target values. The model is evaluated by calculating metrics such as Root Mean Squared Error (RMSE) and R², which measure the performance and fit of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression.\n",
    "# The learning objective is to minimize the specified loss function, with regularization.\n",
    "#More about regParam and elasticNetParam (seen as penalties on the loss function that minimises error in predicitons) can be found here: https://runawayhorse001.github.io/LearningApacheSpark/reg.html\n",
    "# The maxIter parameter sets an upper limit on the number of iterations the optimization algorithm can perform regarding the loss function\n",
    "# By default the LinearRegression function expects another column named \"features\"\n",
    "\n",
    "# Initialize the LinearRegression model with specified parameters\n",
    "# The labelCol parameter specifies the target column 'load' and maxIter sets the max iterations for optimization\n",
    "lr = LinearRegression(labelCol=\"load\", maxIter=500)\n",
    "\n",
    "# Train the model using the training data\n",
    "linearModel = lr.fit(train_data)\n",
    "\n",
    "# Get the summary of the trained model\n",
    "trainingSummary = linearModel.summary\n",
    "\n",
    "# Print the performance metrics: RMSE (Root Mean Squared Error) and R² (R-squared)\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)  # RMSE measures the average magnitude of errors\n",
    "print(\"r2: %f\" % trainingSummary.r2)  # R² is the proportion of the variance in the dependent variable that is predictable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"load\", maxIter=500)\n",
    "\n",
    "# Train the Linear Regression model\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"load\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "lr_rmse = evaluator.evaluate(lr_predictions)\n",
    "\n",
    "print(10 * \"-\")\n",
    "print(\"LinearRegression\")\n",
    "print(f\"RMSE: {lr_rmse}\")\n",
    "print(f\"r2: {lr_model.summary.r2}\")\n",
    "print(10 * \"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "In this section, a Random Forest model is trained on the dataset and evaluated based on performance metrics such as RMSE (Root Mean Squared Error) and R² (R-squared). These metrics help assess the model's accuracy and its ability to explain the variance in the data.\n",
    "\n",
    "We chose a random Forest because it is a powerful machine learning algorithm because it combines the predictions of multiple decision trees, making it less prone to overfitting and providing robust predictions. By averaging the outputs of many trees, it reduces the variance compared to individual decision trees, leading to more stable and accurate predictions. Random Forest also works well with non-linear relationships and can handle a large number of input features, making it a versatile and reliable model for regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Random Forest model with parameters such as number of trees (numTrees), maximum tree depth (maxDepth), and a random seed for reproducibility\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"load\", numTrees=100, maxDepth=10, seed=4343)\n",
    "\n",
    "# Train the Random Forest model using the training dataset\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Use the trained model to make predictions on the test dataset\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Initialize an evaluator to compute RMSE for the model's performance on predictions\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"load\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rf_rmse = evaluator_rmse.evaluate(rf_predictions)\n",
    "\n",
    "# Initialize an evaluator to compute R² for the model's performance on predictions\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"load\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "rf_r2 = evaluator_r2.evaluate(rf_predictions)\n",
    "\n",
    "# Output the performance metrics for Random Forest\n",
    "print(10 * \"-\")\n",
    "print(\"RandomForestRegressor\")\n",
    "print(f\"RMSE: {rf_rmse}\")  # Print RMSE (lower is better)\n",
    "print(f\"r2: {rf_r2}\")  # Print R² (higher is better)\n",
    "print(10 * \"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation\n",
    "\n",
    "This code compares the performance of the Random Forest and Linear Regression models by visualizing both the predicted versus actual values and the residuals (errors) for each model. The first plot is a scatter plot that shows how closely the predicted values from both models align with the actual load values. A \"Perfect Prediction Line\" is added to the plot, which represents where the predicted values would lie if they matched the actual values exactly. Points closer to this line indicate better predictions.\n",
    "\n",
    "The second plot displays the distribution of residuals, which are calculated as the difference between the actual load values and the predicted values. The histograms for both models show how the residuals are spread. Ideally, the residuals should be centered around zero, indicating that the models have no systematic bias. A red dashed line is drawn at zero, helping to assess how well the models are predicting—residuals close to zero suggest better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert Spark DataFrames to Pandas for easier plotting\n",
    "rf_predictions_pd = rf_predictions.select(\"load\", \"prediction\").toPandas()\n",
    "lr_predictions_pd = lr_predictions.select(\"load\", \"prediction\").toPandas()\n",
    "\n",
    "# Scatter plot of actual vs predicted values for both models\n",
    "plt.figure(figsize=(10, 5))  # Set the figure size for the plot\n",
    "# Scatter plot for Random Forest predictions\n",
    "plt.scatter(rf_predictions_pd[\"load\"], rf_predictions_pd[\"prediction\"], alpha=0.5, label=\"Random Forest Predictions\", color='blue')\n",
    "# Scatter plot for Linear Regression predictions\n",
    "plt.scatter(lr_predictions_pd[\"load\"], lr_predictions_pd[\"prediction\"], alpha=0.5, label=\"Linear Regression Predictions\", color='green')\n",
    "# Add a red dashed line representing the perfect prediction line (where predicted values = actual values)\n",
    "plt.plot(\n",
    "    [rf_predictions_pd[\"load\"].min(), rf_predictions_pd[\"load\"].max()],  # X-axis range: min to max of actual load\n",
    "    [rf_predictions_pd[\"load\"].min(), rf_predictions_pd[\"load\"].max()],  # Y-axis range: min to max of actual load\n",
    "    color=\"red\",  # Color of the line\n",
    "    linestyle=\"--\",  # Dashed line style\n",
    "    label=\"Perfect Prediction Line\"  # Label for the line in the legend\n",
    ")\n",
    "# Set labels for the x and y axes\n",
    "plt.xlabel(\"Actual Load\")\n",
    "plt.ylabel(\"Predicted Load\")\n",
    "# Set the title of the plot\n",
    "plt.title(\"Predicted vs Actual Load (RF vs LR)\")\n",
    "# Display the legend for the two models and the perfect prediction line\n",
    "plt.legend()\n",
    "# Enable grid for better readability of the plot\n",
    "plt.grid(True)\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Residuals histogram for both models\n",
    "# Calculate residuals (difference between actual and predicted values) for Random Forest\n",
    "rf_residuals = rf_predictions_pd[\"load\"] - rf_predictions_pd[\"prediction\"]\n",
    "# Calculate residuals for Linear Regression\n",
    "lr_residuals = lr_predictions_pd[\"load\"] - lr_predictions_pd[\"prediction\"]\n",
    "\n",
    "# Create a histogram to visualize the residuals of both models\n",
    "plt.figure(figsize=(10, 5))  # Set figure size for the histogram\n",
    "# Histogram for Random Forest residuals\n",
    "plt.hist(rf_residuals, bins=30, alpha=0.5, label=\"Random Forest Residuals\", color='blue', edgecolor='k')\n",
    "# Histogram for Linear Regression residuals\n",
    "plt.hist(lr_residuals, bins=30, alpha=0.5, label=\"Linear Regression Residuals\", color='green', edgecolor='k')\n",
    "# Add a red dashed line at zero to represent zero error (ideal point)\n",
    "plt.axvline(0, color=\"red\", linestyle=\"--\", label=\"Zero Error Line\")\n",
    "# Set the labels for the x and y axes\n",
    "plt.xlabel(\"Residuals (Actual - Predicted)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "# Set the title for the histogram\n",
    "plt.title(\"Residuals Distribution (RF vs LR)\")\n",
    "# Display the legend for the two models and the zero error line\n",
    "plt.legend()\n",
    "# Enable grid for better readability\n",
    "plt.grid(True)\n",
    "# Show the histogram plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
