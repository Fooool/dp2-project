{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harnessing Weather Insights for Accurate Energy Load Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize via the full spark path\n",
    "if platform.system() == 'Windows':\n",
    "    print(\"Windows OS detected\")\n",
    "    findspark.init(\"C:/Spark/spark-3.5.4-bin-hadoop3\") # For my local machine\n",
    "else:\n",
    "    findspark.init(\"/usr/local/spark/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local\") \\\n",
    "      .appName(\"Linear Regression Model\") \\\n",
    "      .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "      .config(\"spark.jars.packages\", \"com.databricks:spark-xml_2.12:0.15.0\") \\\n",
    "      .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "      .getOrCreate()\n",
    "   \n",
    "# Main entry point for Spark functionality. A SparkContext represents the\n",
    "# connection to a Spark cluster, and can be used to create :class:`RDD` and\n",
    "# broadcast variables on that cluster.      \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing I: Read in Weather and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in data\\geosphere\\2024\\01.csv\n",
      "1490\n",
      "Reading in data\\geosphere\\2024\\02.csv\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `time` cannot be resolved. Did you mean one of the following? [` bewm_mittel`, ` vv_mittel' not available or access denied\"}`, `{\"error\":\"Violation for parameters: 'tl_mittel`].;\n'Project [{\"error\":\"Violation for parameters: 'tl_mittel#57545,  bewm_mittel#57546,  vv_mittel' not available or access denied\"}#57547, to_timestamp('time, Some(yyyy-MM-dd'T'HH:mmXXX), TimestampType, Some(UTC), false) AS time#57551]\n+- Relation [{\"error\":\"Violation for parameters: 'tl_mittel#57545, bewm_mittel#57546, vv_mittel' not available or access denied\"}#57547] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[119], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;28mstr\u001b[39m(month_file), header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Convert the time column (string) to a timestamp\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_timestamp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myyyy-MM-dd\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHH:mmXXX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m dfs\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mcount())\n",
      "File \u001b[1;32mp:\\Development\\VS\\dp2-project\\.venv\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:5176\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[1;34m(self, colName, col)\u001b[0m\n\u001b[0;32m   5171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[0;32m   5172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m   5173\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5174\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m   5175\u001b[0m     )\n\u001b[1;32m-> 5176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32mp:\\Development\\VS\\dp2-project\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mp:\\Development\\VS\\dp2-project\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `time` cannot be resolved. Did you mean one of the following? [` bewm_mittel`, ` vv_mittel' not available or access denied\"}`, `{\"error\":\"Violation for parameters: 'tl_mittel`].;\n'Project [{\"error\":\"Violation for parameters: 'tl_mittel#57545,  bewm_mittel#57546,  vv_mittel' not available or access denied\"}#57547, to_timestamp('time, Some(yyyy-MM-dd'T'HH:mmXXX), TimestampType, Some(UTC), false) AS time#57551]\n+- Relation [{\"error\":\"Violation for parameters: 'tl_mittel#57545, bewm_mittel#57546, vv_mittel' not available or access denied\"}#57547] csv\n"
     ]
    }
   ],
   "source": [
    "# Read in the data\n",
    "\n",
    "# Folder Structure\n",
    "# data\n",
    "# |-- geosphere\n",
    "# |   |-- YYYY\n",
    "# |      |-- MM.csv\n",
    "# |      |-- MM.csv\n",
    "# |\n",
    "# |-- transparency\n",
    "# |   |-- YYYY\n",
    "# |      |-- MM.xml\n",
    "# |      |-- MM.xml\n",
    "\n",
    "# Loop through the geosphere folder and read in the data\n",
    "\n",
    "# Define the base data folder\n",
    "base_path = Path(\"./data/geosphere\")\n",
    "\n",
    "# Collect all data frames first to optimize the union operation\n",
    "dfs = []\n",
    "\n",
    "for year_folder in base_path.iterdir():\n",
    "    if year_folder.is_dir():\n",
    "        for month_file in year_folder.glob(\"*.csv\"):\n",
    "            print(f\"Reading in {month_file}\")\n",
    "\n",
    "            df = spark.read.csv(str(month_file), header=True, inferSchema=True)\n",
    "\n",
    "            # Convert the time column (string) to a timestamp\n",
    "            df = df.withColumn(\"time\", to_timestamp(col(\"time\"), \"yyyy-MM-dd'T'HH:mmXXX\"))\n",
    "            \n",
    "            dfs.append(df)\n",
    "            \n",
    "            print(df.count())\n",
    "\n",
    "if dfs:\n",
    "    # Combine all DataFrames\n",
    "    weather = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        weather = weather.union(df)\n",
    "\n",
    "    # Aggregate measurements (average from different stations)\n",
    "    weather = (\n",
    "        weather.groupBy(\"time\")\n",
    "        .agg(\n",
    "            avg(\"rr\").alias(\"avg_rr\"),\n",
    "            avg(\"tl\").alias(\"avg_tl\"),\n",
    "            avg(\"p\").alias(\"avg_p\"),\n",
    "            avg(\"so_h\").alias(\"avg_so_h\"),\n",
    "            avg(\"ff\").alias(\"avg_ff\"),\n",
    "        )\n",
    "        .orderBy(\"time\")\n",
    "    )\n",
    "\n",
    "    weather.show(10, truncate=False)\n",
    "    \n",
    "    print(weather.count())\n",
    "    weather.printSchema()\n",
    "else:\n",
    "    print(\"No data found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading transparency data: data\\transparency\\2024\\01.xml\n",
      "Reading transparency data: data\\transparency\\2024\\02.xml\n",
      "Reading transparency data: data\\transparency\\2024\\03.xml\n",
      "Reading transparency data: data\\transparency\\2024\\04.xml\n",
      "Reading transparency data: data\\transparency\\2024\\05.xml\n",
      "Reading transparency data: data\\transparency\\2024\\06.xml\n",
      "Reading transparency data: data\\transparency\\2024\\07.xml\n",
      "Reading transparency data: data\\transparency\\2024\\08.xml\n",
      "Reading transparency data: data\\transparency\\2024\\09.xml\n",
      "Reading transparency data: data\\transparency\\2024\\10.xml\n",
      "Reading transparency data: data\\transparency\\2024\\11.xml\n",
      "Reading transparency data: data\\transparency\\2024\\12.xml\n",
      "+-------------------+--------+\n",
      "|        actual_time|quantity|\n",
      "+-------------------+--------+\n",
      "|2024-01-01 00:00:00|  5578.0|\n",
      "|2024-01-01 00:15:00|  5511.0|\n",
      "|2024-01-01 00:30:00|  5444.0|\n",
      "|2024-01-01 00:45:00|  5390.0|\n",
      "|2024-01-01 01:00:00|  5424.0|\n",
      "|2024-01-01 01:15:00|  5350.0|\n",
      "|2024-01-01 01:30:00|  5290.0|\n",
      "|2024-01-01 01:45:00|  5243.0|\n",
      "|2024-01-01 02:00:00|  5172.0|\n",
      "|2024-01-01 02:15:00|  5106.0|\n",
      "+-------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- actual_time: timestamp (nullable = true)\n",
      " |-- quantity: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through the transparency folder and read in the energy data\n",
    "\n",
    "# Define base path for transparency data\n",
    "base_path = Path(\"./data/transparency\")\n",
    "\n",
    "# Collect DataFrames before performing union (optimization)\n",
    "dfs = []\n",
    "\n",
    "for year_folder in base_path.iterdir():\n",
    "    if year_folder.is_dir():\n",
    "        for month_file in year_folder.glob(\"*.xml\"):\n",
    "            print(f\"Reading transparency data: {month_file}\")\n",
    "\n",
    "            # Read XML data\n",
    "            df = spark.read.format('xml').option(\"rowTag\", \"GL_MarketDocument\").load(str(month_file))\n",
    "\n",
    "            # Extract and explode relevant fields\n",
    "            df_filtered = df.select(\n",
    "                col(\"TimeSeries.Period.timeInterval.start\").alias(\"start_time\"),\n",
    "                col(\"TimeSeries.Period.timeInterval.end\").alias(\"end_time\"),\n",
    "                col(\"TimeSeries.Period.resolution\").alias(\"resolution\"),\n",
    "                explode(col(\"TimeSeries.Period.Point\")).alias(\"Point\")  # Flatten Points\n",
    "            ).select(\n",
    "                col(\"start_time\"),\n",
    "                col(\"end_time\"),\n",
    "                col(\"resolution\"),\n",
    "                col(\"Point.position\").cast(\"int\").alias(\"position\"),\n",
    "                col(\"Point.quantity\").cast(\"double\").alias(\"quantity\")\n",
    "            )\n",
    "\n",
    "            # Convert ISO 8601 duration (e.g., \"PT15M\") to minutes dynamically\n",
    "            df_fixed = df_filtered.withColumn(\n",
    "                \"interval_minutes\",\n",
    "                expr(\"CAST(SUBSTRING(resolution, 3, LENGTH(resolution) - 3) AS INT)\")  # Extracts \"15\" from \"PT15M\"\n",
    "            ).withColumn(\n",
    "                \"actual_time\",\n",
    "                expr(\"start_time + (position - 1) * interval_minutes * interval 1 minute\")\n",
    "            ).select(\n",
    "                col(\"actual_time\"),\n",
    "                col(\"quantity\")\n",
    "            )\n",
    "\n",
    "            # Append DataFrame to list\n",
    "            dfs.append(df_fixed)\n",
    "\n",
    "# Merge all collected DataFrames\n",
    "if dfs:\n",
    "    Load = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        Load = Load.union(df)\n",
    "\n",
    "    Load.show(10)\n",
    "    Load.printSchema()\n",
    "else:\n",
    "    print(\"No data found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing II: Combine both Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+------------------+------------------+------------------+\n",
      "|summary|              load|         rainfall|       temperature|        cloudiness| sunshine_duration|        wind_speed|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+------------------+\n",
      "|  count|               366|              366|               366|               366|               366|               366|\n",
      "|   mean|5465.7213114754095|2.366666666666667|11.837295081967213| 61.48497267759563|5.1245901639344265|2.3968579234972673|\n",
      "| stddev| 662.4324448860633|7.928408835741675| 7.767922876423139|23.567110083132476|3.6014679992167693|1.0763445069155058|\n",
      "|    min|            4386.0|             -1.0|              -6.1|               0.0|               0.0|               0.5|\n",
      "|    max|            7270.0|82.69999999999999|              26.5|             100.0|             13.55|               8.3|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+------------------+\n",
      "\n",
      "+------+-------------------+------------------+------------------+----------+-----------------+------------------+\n",
      "|  load|               time|          rainfall|       temperature|cloudiness|sunshine_duration|        wind_speed|\n",
      "+------+-------------------+------------------+------------------+----------+-----------------+------------------+\n",
      "|5578.0|2024-01-01 00:00:00|               0.2|               4.0|      66.5|             3.35|               3.3|\n",
      "|5429.0|2024-01-02 00:00:00|               3.8|2.1999999999999997|      85.0|             0.95|              1.35|\n",
      "|5747.0|2024-01-03 00:00:00|               0.0|6.1000000000000005|      50.0|             6.75|              3.25|\n",
      "|5641.0|2024-01-04 00:00:00|               0.2|               5.7|      50.0|             4.75|6.3999999999999995|\n",
      "|5803.0|2024-01-05 00:00:00|              2.25|               4.8|      68.5|             2.15|              1.35|\n",
      "|5706.0|2024-01-06 00:00:00|              13.3|              2.55|     100.0|              0.0|               1.6|\n",
      "|5688.0|2024-01-07 00:00:00|1.4000000000000001|               1.3|      95.0|              0.0|              2.55|\n",
      "|6348.0|2024-01-08 00:00:00|               0.4|             -2.85|      96.5|              0.0|               4.0|\n",
      "|7090.0|2024-01-09 00:00:00|              -1.0|              -6.1|      46.5|              2.6|              1.35|\n",
      "|7102.0|2024-01-10 00:00:00|              -1.0|              -4.1|      10.0|             7.15|              2.35|\n",
      "+------+-------------------+------------------+------------------+----------+-----------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- load: double (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- rainfall: double (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- cloudiness: double (nullable = true)\n",
      " |-- sunshine_duration: double (nullable = true)\n",
      " |-- wind_speed: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if Load is not None and weather is not None:\n",
    "    # Join the data into a single DataFrame\n",
    "    data = Load.join(weather, Load.actual_time == weather.time, \"inner\").drop(\"time\")\n",
    "    \n",
    "    # Rename columns for better understanding\n",
    "    data = data.withColumnRenamed(\"actual_time\", \"time\")\n",
    "    data = data.withColumnRenamed(\"quantity\", \"load\")\n",
    "\n",
    "    data = data.withColumnRenamed(\"avg_rr\", \"rainfall\")\n",
    "    data = data.withColumnRenamed(\"avg_tl_mittel\", \"temperature\")\n",
    "    data = data.withColumnRenamed(\"avg_bewm_mittel\", \"cloudiness\")\n",
    "    data = data.withColumnRenamed(\"avg_so_h\", \"sunshine_duration\")\n",
    "    data = data.withColumnRenamed(\"avg_vv_mittel\", \"wind_speed\")\n",
    "    \n",
    "    # Reorder the columns\n",
    "    data = data.select(\"load\", \"time\", \"rainfall\", \"temperature\", \"cloudiness\", \"sunshine_duration\", \"wind_speed\")\n",
    "    \n",
    "    # Print the schema and stats\n",
    "    data.describe().show()\n",
    "    data.show(10)\n",
    "    data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366\n"
     ]
    }
   ],
   "source": [
    "print(data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
