{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harnessing Weather Insights for Accurate Energy Load Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize via the full spark path\n",
    "if platform.system() == 'Windows':\n",
    "    print(\"Windows OS detected\")\n",
    "    findspark.init(\"C:/Spark/spark-3.5.4-bin-hadoop3\") # For my local machine\n",
    "else:\n",
    "    findspark.init(\"/usr/local/spark/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local\") \\\n",
    "      .appName(\"Linear Regression Model\") \\\n",
    "      .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "      .config(\"spark.jars.packages\", \"com.databricks:spark-xml_2.12:0.15.0\") \\\n",
    "      .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "      .getOrCreate()\n",
    "   \n",
    "# Main entry point for Spark functionality. A SparkContext represents the\n",
    "# connection to a Spark cluster, and can be used to create :class:`RDD` and\n",
    "# broadcast variables on that cluster.      \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing I: Read in Weather and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "\n",
    "# Folder Structure\n",
    "# data\n",
    "# |-- geosphere\n",
    "# |   |-- YYYY\n",
    "# |      |-- MM.csv\n",
    "# |      |-- MM.csv\n",
    "# |\n",
    "# |-- transparency\n",
    "# |   |-- YYYY\n",
    "# |      |-- MM.xml\n",
    "# |      |-- MM.xml\n",
    "\n",
    "# Loop through the geosphere folder and read in the data\n",
    "\n",
    "# Define the base data folder\n",
    "base_path = Path(\"./data/geosphere\")\n",
    "\n",
    "# Collect all data frames first to optimize the union operation\n",
    "dfs = []\n",
    "\n",
    "for year_folder in base_path.iterdir():\n",
    "    if year_folder.is_dir():\n",
    "        for month_file in year_folder.glob(\"*.csv\"):\n",
    "            print(f\"Reading in {month_file}\")\n",
    "\n",
    "            df = spark.read.csv(str(month_file), header=True, inferSchema=True)\n",
    "            \n",
    "            # Convert the time column (string) to a timestamp\n",
    "            df = df.withColumn(\"time\", to_timestamp(col(\"time\"), \"yyyy-MM-dd'T'HH:mmXXX\"))\n",
    "\n",
    "            dfs.append(df)\n",
    "\n",
    "if dfs:\n",
    "    # Combine all DataFrames\n",
    "    weather = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        weather = weather.union(df)\n",
    "\n",
    "    # Aggregate measurements (average from different stations)\n",
    "    weather = (\n",
    "        weather.groupBy(\"time\")\n",
    "        .agg(\n",
    "            avg(\"rr\").alias(\"avg_rr\"),\n",
    "            avg(\"tl_mittel\").alias(\"avg_tl_mittel\"),\n",
    "            avg(\"bewm_mittel\").alias(\"avg_bewm_mittel\"),\n",
    "            avg(\"so_h\").alias(\"avg_so_h\"),\n",
    "            avg(\"vv_mittel\").alias(\"avg_vv_mittel\"),\n",
    "        )\n",
    "        .orderBy(\"time\")\n",
    "    )\n",
    "\n",
    "    weather.show(10)\n",
    "    weather.printSchema()\n",
    "else:\n",
    "    print(\"No data found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the transparency folder and read in the energy data\n",
    "\n",
    "# Define base path for transparency data\n",
    "base_path = Path(\"./data/transparency\")\n",
    "\n",
    "# Collect DataFrames before performing union (optimization)\n",
    "dfs = []\n",
    "\n",
    "for year_folder in base_path.iterdir():\n",
    "    if year_folder.is_dir():\n",
    "        for month_file in year_folder.glob(\"*.xml\"):\n",
    "            print(f\"Reading transparency data: {month_file}\")\n",
    "\n",
    "            # Read XML data\n",
    "            df = spark.read.format('xml').option(\"rowTag\", \"GL_MarketDocument\").load(str(month_file))\n",
    "\n",
    "            # Extract and explode relevant fields\n",
    "            df_filtered = df.select(\n",
    "                col(\"TimeSeries.Period.timeInterval.start\").alias(\"start_time\"),\n",
    "                col(\"TimeSeries.Period.timeInterval.end\").alias(\"end_time\"),\n",
    "                col(\"TimeSeries.Period.resolution\").alias(\"resolution\"),\n",
    "                explode(col(\"TimeSeries.Period.Point\")).alias(\"Point\")  # Flatten Points\n",
    "            ).select(\n",
    "                col(\"start_time\"),\n",
    "                col(\"end_time\"),\n",
    "                col(\"resolution\"),\n",
    "                col(\"Point.position\").cast(\"int\").alias(\"position\"),\n",
    "                col(\"Point.quantity\").cast(\"double\").alias(\"quantity\")\n",
    "            )\n",
    "\n",
    "            # Convert ISO 8601 duration (e.g., \"PT15M\") to minutes dynamically\n",
    "            df_fixed = df_filtered.withColumn(\n",
    "                \"interval_minutes\",\n",
    "                expr(\"CAST(SUBSTRING(resolution, 3, LENGTH(resolution) - 3) AS INT)\")  # Extracts \"15\" from \"PT15M\"\n",
    "            ).withColumn(\n",
    "                \"actual_time\",\n",
    "                expr(\"start_time + (position - 1) * interval_minutes * interval 1 minute\")\n",
    "            ).select(\n",
    "                col(\"actual_time\"),\n",
    "                col(\"quantity\")\n",
    "            )\n",
    "\n",
    "            # Append DataFrame to list\n",
    "            dfs.append(df_fixed)\n",
    "\n",
    "# Merge all collected DataFrames\n",
    "if dfs:\n",
    "    Load = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        Load = Load.union(df)\n",
    "\n",
    "    Load.show(10)\n",
    "    Load.printSchema()\n",
    "else:\n",
    "    print(\"No data found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing II: Combine both Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Load is not None and weather is not None:\n",
    "    # Join the data into a single DataFrame\n",
    "    data = Load.join(weather, Load.actual_time == weather.time, \"inner\").drop(\"time\")\n",
    "    \n",
    "    # Rename columns for better understanding\n",
    "    data = data.withColumnRenamed(\"actual_time\", \"time\")\n",
    "    data = data.withColumnRenamed(\"quantity\", \"load\")\n",
    "\n",
    "    data = data.withColumnRenamed(\"avg_rr\", \"rainfall\")\n",
    "    data = data.withColumnRenamed(\"avg_tl_mittel\", \"temperature\")\n",
    "    data = data.withColumnRenamed(\"avg_bewm_mittel\", \"cloudiness\")\n",
    "    data = data.withColumnRenamed(\"avg_so_h\", \"sunshine_duration\")\n",
    "    data = data.withColumnRenamed(\"avg_vv_mittel\", \"wind_speed\")\n",
    "    \n",
    "    # Print the schema and stats\n",
    "    data.describe().show()\n",
    "    data.show(10)\n",
    "    data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = data.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "\n",
    "# Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
    "df_ml = spark.createDataFrame(input_data, [\"label\", \"features\"])\n",
    "\n",
    "# Prints the first row to the console.\n",
    "df_ml.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
