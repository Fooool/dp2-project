{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harnessing Weather Insights for Accurate Energy Load Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize via the full spark path\n",
    "if platform.system() == 'Windows':\n",
    "    print(\"Windows OS detected\")\n",
    "    findspark.init(\"C:/Spark/spark-3.5.4-bin-hadoop3\") # For my local machine\n",
    "else:\n",
    "    findspark.init(\"/usr/local/spark/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Linear Regression Model\") \\\n",
    "    .config(\"spark.executor.memory\", \"32g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.databricks:spark-xml_2.12:0.15.0\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"100s\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "   \n",
    "# Main entry point for Spark functionality. A SparkContext represents the\n",
    "# connection to a Spark cluster, and can be used to create :class:`RDD` and\n",
    "# broadcast variables on that cluster.      \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing I: Read in Weather and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "\n",
    "# Folder Structure\n",
    "# data\n",
    "# |-- geosphere\n",
    "# |   |-- YYYY\n",
    "# |      |-- MM.csv\n",
    "# |      |-- MM.csv\n",
    "# |\n",
    "# |-- transparency\n",
    "# |   |-- YYYY\n",
    "# |      |-- MM.xml\n",
    "# |      |-- MM.xml\n",
    "\n",
    "# Loop through the geosphere folder and read in the data\n",
    "\n",
    "# Define the base data folder\n",
    "base_path = Path(\"./data/geosphere\")\n",
    "\n",
    "# Collect all data frames first to optimize the union operation\n",
    "dfs = []\n",
    "\n",
    "for year_folder in base_path.iterdir():\n",
    "    if year_folder.is_dir():\n",
    "        for month_file in year_folder.glob(\"*.csv\"):\n",
    "            print(f\"Reading in {month_file}\")\n",
    "\n",
    "            df = spark.read.csv(str(month_file), header=True, inferSchema=True)\n",
    "\n",
    "            # Convert the time column (string) to a timestamp\n",
    "            df = df.withColumn(\"time\", to_timestamp(col(\"time\"), \"yyyy-MM-dd'T'HH:mmXXX\"))\n",
    "            \n",
    "            dfs.append(df)\n",
    "            \n",
    "            print(f\"Read in {df.count()} rows\")\n",
    "\n",
    "if dfs:\n",
    "    # Combine all DataFrames\n",
    "    weather = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        weather = weather.union(df)\n",
    "\n",
    "    # Aggregate measurements (average from different stations)\n",
    "    weather = (\n",
    "        weather.groupBy(\"time\")\n",
    "        .agg(\n",
    "            avg(\"rr\").alias(\"avg_rr\"),\n",
    "            avg(\"tl\").alias(\"avg_tl\"),\n",
    "            avg(\"p\").alias(\"avg_p\"),\n",
    "            avg(\"so_h\").alias(\"avg_so_h\"),\n",
    "            avg(\"ff\").alias(\"avg_ff\"),\n",
    "        )\n",
    "        .orderBy(\"time\")\n",
    "    )\n",
    "\n",
    "    # Log-transform the measurements (data for rainfall and windspeed is skewed)\n",
    "    weather = weather.withColumn(\"log_rr\", log1p(col(\"avg_rr\")))\n",
    "    weather = weather.withColumn(\"log_ff\", log1p(col(\"avg_ff\")))\n",
    "    \n",
    "    weather.drop(\"avg_rr\", \"avg_ff\")\n",
    "    \n",
    "    weather.show(10, truncate=False)\n",
    "    \n",
    "    print(weather.count())\n",
    "    weather.printSchema()\n",
    "else:\n",
    "    print(\"No data found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the transparency folder and read in the energy data\n",
    "\n",
    "# Define base path for transparency data\n",
    "base_path = Path(\"./data/transparency\")\n",
    "\n",
    "# Collect DataFrames before performing union (optimization)\n",
    "dfs = []\n",
    "\n",
    "for year_folder in base_path.iterdir():\n",
    "    if year_folder.is_dir():\n",
    "        for month_file in year_folder.glob(\"*.xml\"):\n",
    "            print(f\"Reading transparency data: {month_file}\")\n",
    "\n",
    "            # Read XML data\n",
    "            df = spark.read.format('xml').option(\"rowTag\", \"GL_MarketDocument\").load(str(month_file))\n",
    "\n",
    "            # Extract and explode relevant fields\n",
    "            df_filtered = df.select(\n",
    "                col(\"TimeSeries.Period.timeInterval.start\").alias(\"start_time\"),\n",
    "                col(\"TimeSeries.Period.timeInterval.end\").alias(\"end_time\"),\n",
    "                col(\"TimeSeries.Period.resolution\").alias(\"resolution\"),\n",
    "                explode(col(\"TimeSeries.Period.Point\")).alias(\"Point\")  # Flatten Points\n",
    "            ).select(\n",
    "                col(\"start_time\"),\n",
    "                col(\"end_time\"),\n",
    "                col(\"resolution\"),\n",
    "                col(\"Point.position\").cast(\"int\").alias(\"position\"),\n",
    "                col(\"Point.quantity\").cast(\"double\").alias(\"quantity\")\n",
    "            )\n",
    "\n",
    "            # Convert ISO 8601 duration (e.g., \"PT15M\") to minutes dynamically\n",
    "            df_fixed = df_filtered.withColumn(\n",
    "                \"interval_minutes\",\n",
    "                expr(\"CAST(SUBSTRING(resolution, 3, LENGTH(resolution) - 3) AS INT)\")  # Extracts \"15\" from \"PT15M\"\n",
    "            ).withColumn(\n",
    "                \"actual_time\",\n",
    "                expr(\"start_time + (position - 1) * interval_minutes * interval 1 minute\")\n",
    "            ).select(\n",
    "                col(\"actual_time\"),\n",
    "                col(\"quantity\")\n",
    "            )\n",
    "            \n",
    "            # Aggregate to hourly intervals\n",
    "            df_hourly = df_fixed.withColumn(\n",
    "                \"hourly_time\", date_trunc(\"hour\", col(\"actual_time\"))  # Round down to the hour\n",
    "            ).groupBy(\"hourly_time\").agg(\n",
    "                sum(\"quantity\").alias(\"hourly_quantity\")  # Sum all sub-hourly measurements\n",
    "            )\n",
    "\n",
    "            # Rename to match the structure of other time series\n",
    "            df_hourly = df_hourly.select(\n",
    "                col(\"hourly_time\").alias(\"timestamp\"), col(\"hourly_quantity\").alias(\"quantity\")\n",
    "            ).orderBy(\"timestamp\")\n",
    "            \n",
    "            print(f\"Read in {df_hourly.count()} values\")\n",
    "            \n",
    "            # Append DataFrame to list\n",
    "            dfs.append(df_hourly)\n",
    "\n",
    "# Merge all collected DataFrames\n",
    "if dfs:\n",
    "    Load = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        Load = Load.union(df)\n",
    "\n",
    "    Load.show(10)\n",
    "    Load.printSchema()\n",
    "else:\n",
    "    print(\"No data found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing II: Combine both Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Load is not None and weather is not None:    \n",
    "    # Join the data into a single DataFrame    \n",
    "    data = Load.join(weather, Load.timestamp == weather.time, \"inner\").drop(\"time\")\n",
    "    \n",
    "    # Rename columns for better understanding\n",
    "    data = data.withColumnRenamed(\"timestamp\", \"time\")\n",
    "    data = data.withColumnRenamed(\"quantity\", \"load\")\n",
    "\n",
    "    data = data.withColumnRenamed(\"log_rr\", \"rainfall\")\n",
    "    data = data.withColumnRenamed(\"avg_tl\", \"temperature\")\n",
    "    data = data.withColumnRenamed(\"avg_p\", \"pressure\")\n",
    "    data = data.withColumnRenamed(\"avg_so_h\", \"sunshine_duration\")\n",
    "    data = data.withColumnRenamed(\"log_ff\", \"wind_speed\")\n",
    "    \n",
    "    # Add Lag features\n",
    "    \n",
    "    # Define window for lagging (ordered by time)\n",
    "    window_spec = Window().partitionBy().orderBy(\"time\")\n",
    "\n",
    "    # Add lag features (previous hour's values)\n",
    "    data = data.withColumn(\"rainfall_lag_1\", lag(\"rainfall\", 1).over(window_spec))\n",
    "    data = data.withColumn(\"wind_speed_lag_1\", lag(\"wind_speed\", 1).over(window_spec))\n",
    "    \n",
    "    # Add lag features (7 previous days values)\n",
    "    days = []\n",
    "    for i in range(24, 24*7+1, 24):\n",
    "        data = data.withColumn(f\"load_lag_{i}h_prev_day\", lag(\"load\", i).over(window_spec))\n",
    "        days.append(f\"load_lag_{i}h_prev_day\")\n",
    "    \n",
    "    # Drop rows with missing values\n",
    "    data = data.dropna()\n",
    "    \n",
    "    # Reorder the columns\n",
    "    data = data.select(\"time\",\n",
    "                        \"load\", \n",
    "                        \"rainfall\", \n",
    "                        \"rainfall_lag_1\",\n",
    "                        \"temperature\", \n",
    "                        \"pressure\", \n",
    "                        \"sunshine_duration\", \n",
    "                        \"wind_speed\", \n",
    "                        \"wind_speed_lag_1\",\n",
    "                        *days)\n",
    "    \n",
    "    # Print the schema and stats\n",
    "    data.describe().show()\n",
    "    data.show(10)\n",
    "    data.printSchema()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep original timestamp before converting to Unix seconds\n",
    "unix_time_data = data.withColumn(\"unix_time\", unix_timestamp(\"time\"))  # Store Unix time separately\n",
    "\n",
    "# Extract day of the year (1-365/366) from the original timestamp column\n",
    "unix_time_data = unix_time_data.withColumn(\"day_of_year\", dayofyear(col(\"time\")))\n",
    "\n",
    "# Extract time of day in seconds (seconds since midnight)\n",
    "unix_time_data = unix_time_data.withColumn(\n",
    "    \"time_of_day\", expr(\"hour(time) * 3600 + minute(time) * 60 + second(time)\")\n",
    ")\n",
    "\n",
    "# Normalize day of the year to range (0, 2Ï€) for periodic encoding\n",
    "ml_data = unix_time_data.withColumn(\"day_cos\", cos(2 * 3.1416 * col(\"day_of_year\") / 365))\n",
    "\n",
    "# Normalize time of day\n",
    "ml_data = ml_data.withColumn(\"time_cos\", cos(2 * 3.1416 * col(\"time_of_day\") / 86400))\n",
    "\n",
    "# Define new feature columns\n",
    "#feature_columns = [\"day_cos\", \"time_cos\", \"is_weekend\", \"rainfall\", \"temperature\", \"pressure\", \"sunshine_duration\", \"wind_speed\"]\n",
    "\n",
    "feature_columns = [     \"day_cos\",\n",
    "                        \"time_cos\",\n",
    "                        \"rainfall\", \n",
    "                        \"rainfall_lag_1\",\n",
    "                        \"temperature\", \n",
    "                        \"pressure\", \n",
    "                        \"sunshine_duration\", \n",
    "                        \"wind_speed\", \n",
    "                        \"wind_speed_lag_1\", \n",
    "                        \"load_lag_6h_prev_day\",\n",
    "                        \"load_lag_12h_prev_day\", \n",
    "                        \"load_lag_18h_prev_day\",\n",
    "                        \"load_lag_24h_prev_day\"\n",
    "                        ]\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "\n",
    "# Transform data\n",
    "df_ml = assembler.transform(ml_data).select(\"load\", \"features\")\n",
    "\n",
    "df_ml.cache()\n",
    "\n",
    "# Show transformed data\n",
    "df_ml.show(5, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "scaled_df = scaler.fit(df_ml).transform(df_ml).select(\"load\", \"features_scaled\")\n",
    "scaled_df = scaled_df.withColumnRenamed(\"features_scaled\", \"features\")\n",
    "scaled_df.cache()\n",
    "\n",
    "scaled_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly splits this :class:`DataFrame` with the provided weights.\n",
    "train_data, test_data = scaled_df.randomSplit([.8,.2],seed=1234)\n",
    "\n",
    "train_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression.\n",
    "# The learning objective is to minimize the specified loss function, with regularization.\n",
    "#More about regParam and elasticNetParam (seen as penalties on the loss function that minimises error in predicitons) can be found here: https://runawayhorse001.github.io/LearningApacheSpark/reg.html\n",
    "# The maxIter parameter sets an upper limit on the number of iterations the optimization algorithm can perform regarding the loss function\n",
    "# By default the LinearRegression function expects another column named \"features\"\n",
    "\n",
    "lr = LinearRegression(labelCol=\"load\", maxIter=500)\n",
    "\n",
    "# Fits a model to the input dataset with optional parameters.\n",
    "linearModel = lr.fit(train_data)\n",
    "\n",
    "# Get the summary of the model\n",
    "trainingSummary = linearModel.summary\n",
    "\n",
    "# Print the Score of the model\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"load\")\n",
    "\n",
    "# Define hyperparameter grid\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.001, 0.01, 0.1, 1.0])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])  # 0 = Ridge, 1 = Lasso\n",
    "             .build())\n",
    "\n",
    "# Define evaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"load\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Define cross-validation\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Fit the model with cross-validation\n",
    "cv_model = cv.fit(train_data)\n",
    "\n",
    "# Get the best model\n",
    "best_model = cv_model.bestModel\n",
    "print(f\"Best regParam: {best_model._java_obj.getRegParam()}\")\n",
    "print(f\"Best elasticNetParam: {best_model._java_obj.getElasticNetParam()}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = best_model.transform(test_data)\n",
    "\n",
    "# Evaluate performance\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "# Show predictions\n",
    "predictions.select(\"load\", \"prediction\").show(10, truncate=False)\n",
    "\n",
    "r2 = evaluator.setMetricName(\"r2\").evaluate(predictions)\n",
    "print(f\"R2: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get coefficients\n",
    "coefficients = best_model.coefficients.toArray()\n",
    "\n",
    "# Create a DataFrame for feature importance\n",
    "coef_df = pd.DataFrame({\"Feature\": feature_columns, \"Coefficient\": coefficients})\n",
    "\n",
    "# Sort by absolute coefficient values (fixing the issue)\n",
    "coef_df = coef_df.reindex(coef_df[\"Coefficient\"].abs().sort_values(ascending=False).index)\n",
    "\n",
    "# Display feature importance\n",
    "print(\"\\nFeature Importance (Lasso Regression Coefficients):\")\n",
    "print(coef_df)\n",
    "\n",
    "# Identify removed features (Lasso sets them to exactly 0)\n",
    "removed_features = coef_df[coef_df[\"Coefficient\"] == 0][\"Feature\"].tolist()\n",
    "print(f\"\\nFeatures removed by Lasso: {removed_features}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Define the Random Forest model\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"load\", numTrees=100, maxDepth=10, seed=42)\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Evaluate performance\n",
    "evaluator = RegressionEvaluator(labelCol=\"load\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rf_rmse = evaluator.evaluate(rf_predictions)\n",
    "\n",
    "print(f\"Random Forest RMSE: {rf_rmse}\")\n",
    "\n",
    "# Show predictions\n",
    "rf_predictions.select(\"load\", \"prediction\").show(10, truncate=False)\n",
    "\n",
    "# Feature importance\n",
    "rf_feature_importance = rf_model.featureImportances\n",
    "print(\"\\nFeature Importances (Random Forest):\")\n",
    "for i, imp in enumerate(rf_feature_importance):\n",
    "    print(f\"{feature_columns[i]}: {imp}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
